{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef1c6cfe-320e-496f-82a9-c778e60db5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ æ–‡ä»¶å¤§å°: 3502.22 MB\n",
      "\n",
      "ğŸ” æ•°æ®é¢„è§ˆï¼ˆå‰10è¡Œï¼‰:\n",
      "   user_id  item_id  category_id behavior_type   timestamp\n",
      "0        1  2268318      2520377            pv  1511544070\n",
      "1        1  2333346      2520771            pv  1511561733\n",
      "2        1  2576651       149192            pv  1511572885\n",
      "3        1  3830808      4181361            pv  1511593493\n",
      "4        1  4365585      2520377            pv  1511596146\n",
      "5        1  4606018      2735466            pv  1511616481\n",
      "6        1   230380       411153            pv  1511644942\n",
      "7        1  3827899      2920476            pv  1511713473\n",
      "8        1  3745169      2891509            pv  1511725471\n",
      "9        1  1531036      2920476            pv  1511733732\n",
      "\n",
      "ğŸ“Š åˆ—ä¿¡æ¯:\n",
      "  åˆ—æ•°: 5\n",
      "  åˆ—å: ['user_id', 'item_id', 'category_id', 'behavior_type', 'timestamp']\n",
      "\n",
      "â° æ—¶é—´æˆ³ç¤ºä¾‹ï¼ˆå‰5ä¸ªï¼‰: [1511544070, 1511561733, 1511572885, 1511593493, 1511596146]\n",
      "ğŸ¯ è¡Œä¸ºç±»å‹ç¤ºä¾‹: ['pv']\n",
      "\n",
      "ğŸ“ˆ æ­£åœ¨ç»Ÿè®¡æ€»è¡Œæ•°...\n",
      "  æ€»è¡Œæ•°: 100,150,807\n",
      "\n",
      "ğŸ“‹ æ­£åœ¨é‡‡æ ·åˆ†æè¡Œä¸ºåˆ†å¸ƒ...\n",
      "  é‡‡æ ·100,000è¡Œçš„è¡Œä¸ºåˆ†å¸ƒ:\n",
      "    pv: 89,709 è¡Œ (89.7%)\n",
      "    cart: 5,446 è¡Œ (5.4%)\n",
      "    fav: 2,744 è¡Œ (2.7%)\n",
      "    buy: 2,101 è¡Œ (2.1%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# è®¾ç½®æ–‡ä»¶è·¯å¾„\n",
    "file_path = r'D:\\Mycode\\UserBehavior.csv'\n",
    "\n",
    "# 1. ç¡®å®šæ–‡ä»¶å¤§å°å’Œè¡Œæ•°\n",
    "file_size_mb = os.path.getsize(file_path) / (1024*1024)\n",
    "print(f\"ğŸ“ æ–‡ä»¶å¤§å°: {file_size_mb:.2f} MB\")\n",
    "\n",
    "# 2. è¯»å–å‰10è¡Œç¡®è®¤ç»“æ„ï¼ˆå‡è®¾æ— è¡¨å¤´ï¼‰\n",
    "column_names = ['user_id', 'item_id', 'category_id', 'behavior_type', 'timestamp']\n",
    "df_preview = pd.read_csv(file_path, nrows=10, header=None, names=column_names)\n",
    "print(\"\\nğŸ” æ•°æ®é¢„è§ˆï¼ˆå‰10è¡Œï¼‰:\")\n",
    "print(df_preview)\n",
    "print(f\"\\nğŸ“Š åˆ—ä¿¡æ¯:\")\n",
    "print(f\"  åˆ—æ•°: {len(df_preview.columns)}\")\n",
    "print(f\"  åˆ—å: {list(df_preview.columns)}\")\n",
    "\n",
    "# 3. æŸ¥çœ‹æ—¶é—´æˆ³æ ¼å¼å’Œè¡Œä¸ºç±»å‹\n",
    "print(f\"\\nâ° æ—¶é—´æˆ³ç¤ºä¾‹ï¼ˆå‰5ä¸ªï¼‰: {df_preview['timestamp'].head().tolist()}\")\n",
    "print(f\"ğŸ¯ è¡Œä¸ºç±»å‹ç¤ºä¾‹: {df_preview['behavior_type'].unique().tolist()}\")\n",
    "\n",
    "# 4. è·å–æ€»è¡Œæ•°ï¼ˆé«˜æ•ˆæ–¹æ³•ï¼‰\n",
    "print(\"\\nğŸ“ˆ æ­£åœ¨ç»Ÿè®¡æ€»è¡Œæ•°...\")\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    total_lines = sum(1 for _ in f)\n",
    "print(f\"  æ€»è¡Œæ•°: {total_lines:,}\")\n",
    "\n",
    "# 5. é‡‡æ ·åˆ†ææ•°æ®åˆ†å¸ƒ\n",
    "print(\"\\nğŸ“‹ æ­£åœ¨é‡‡æ ·åˆ†æè¡Œä¸ºåˆ†å¸ƒ...\")\n",
    "sample_size = min(100000, total_lines)  # æœ€å¤šé‡‡æ ·10ä¸‡è¡Œ\n",
    "df_sample = pd.read_csv(file_path, nrows=sample_size, header=None, names=column_names)\n",
    "behavior_counts = df_sample['behavior_type'].value_counts()\n",
    "print(f\"  é‡‡æ ·{len(df_sample):,}è¡Œçš„è¡Œä¸ºåˆ†å¸ƒ:\")\n",
    "for behavior, count in behavior_counts.items():\n",
    "    percentage = count / len(df_sample) * 100\n",
    "    print(f\"    {behavior}: {count:,} è¡Œ ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21e117d4-29dc-45cf-9bbe-07eed9fac24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹æµå¼å¤„ç†ï¼ˆå†…å­˜å®‰å…¨ï¼‰...\n",
      "==================================================\n",
      "å·²å¤„ç†å¹¶ä¿å­˜: 500,000 è¡Œ\n",
      "å·²å¤„ç†å¹¶ä¿å­˜: 1,000,000 è¡Œ\n",
      "âœ… æ£€æŸ¥ç‚¹: å·²å®‰å…¨ä¿å­˜ 1,000,000 è¡Œ\n",
      "å·²å¤„ç†å¹¶ä¿å­˜: 1,500,000 è¡Œ\n",
      "å·²å¤„ç†å¹¶ä¿å­˜: 2,000,000 è¡Œ\n",
      "âœ… æ£€æŸ¥ç‚¹: å·²å®‰å…¨ä¿å­˜ 2,000,000 è¡Œ\n",
      "å·²å¤„ç†å¹¶ä¿å­˜: 2,500,000 è¡Œ\n",
      "å·²å¤„ç†å¹¶ä¿å­˜: 3,000,000 è¡Œ\n",
      "âœ… æ£€æŸ¥ç‚¹: å·²å®‰å…¨ä¿å­˜ 3,000,000 è¡Œ\n",
      "å·²å¤„ç†å¹¶ä¿å­˜: 3,500,000 è¡Œ\n",
      "å·²å¤„ç†å¹¶ä¿å­˜: 4,000,000 è¡Œ\n",
      "âœ… æ£€æŸ¥ç‚¹: å·²å®‰å…¨ä¿å­˜ 4,000,000 è¡Œ\n",
      "å·²å¤„ç†å¹¶ä¿å­˜: 4,500,000 è¡Œ\n",
      "å·²å¤„ç†å¹¶ä¿å­˜: 5,000,000 è¡Œ\n",
      "âœ… æ£€æŸ¥ç‚¹: å·²å®‰å…¨ä¿å­˜ 5,000,000 è¡Œ\n",
      "\n",
      "[æµ‹è¯•å®Œæˆ] å·²å®‰å…¨å¤„ç†500ä¸‡è¡Œ\n",
      "==================================================\n",
      "ğŸ‰ æµå¼å¤„ç†å®Œæˆï¼\n",
      "æ•°æ®å·²å®‰å…¨ä¿å­˜è‡³: D:\\Mycode\\processed_data\\user_behavior_clean.csv\n",
      "æ–‡ä»¶å¤§å°: 0.28 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'D:\\\\Mycode\\\\processed_data\\\\user_behavior_clean.csv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def safe_stream_process():\n",
    "    \"\"\"æµå¼å¤„ç†ï¼Œè¾¹è¯»è¾¹å†™ï¼Œå†…å­˜å®‰å…¨\"\"\"\n",
    "    input_path = r'D:\\Mycode\\UserBehavior.csv'\n",
    "    output_path = r'D:\\Mycode\\processed_data\\user_behavior_clean.csv'\n",
    "    \n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    column_names = ['user_id', 'item_id', 'category_id', 'behavior_type', 'timestamp']\n",
    "    chunk_size = 500000  # æ›´å°çš„å—\n",
    "    \n",
    "    print(\"å¼€å§‹æµå¼å¤„ç†ï¼ˆå†…å­˜å®‰å…¨ï¼‰...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    first_batch = True\n",
    "    total_rows = 0\n",
    "    \n",
    "    for i, chunk in enumerate(pd.read_csv(input_path, names=column_names, \n",
    "                                         chunksize=chunk_size)):\n",
    "        # å¤„ç†æ•°æ®\n",
    "        chunk['timestamp'] = pd.to_datetime(chunk['timestamp'], unit='s')\n",
    "        chunk['date'] = chunk['timestamp'].dt.date\n",
    "        chunk['hour'] = chunk['timestamp'].dt.hour\n",
    "        \n",
    "        # ç«‹å³å†™å…¥æ–‡ä»¶ï¼ˆä¸ç§¯ç´¯å†…å­˜ï¼‰\n",
    "        if first_batch:\n",
    "            chunk.to_csv(output_path, index=False, mode='w')\n",
    "            first_batch = False\n",
    "        else:\n",
    "            chunk.to_csv(output_path, index=False, mode='a', header=False)\n",
    "        \n",
    "        total_rows += len(chunk)\n",
    "        print(f\"å·²å¤„ç†å¹¶ä¿å­˜: {total_rows:,} è¡Œ\")\n",
    "        \n",
    "        # æ¯æ‰¹éƒ½ä¿å­˜ï¼Œå³ä½¿å…³æœºä¹Ÿä¸ä¼šä¸¢å¤±\n",
    "        if total_rows % 1000000 == 0:\n",
    "            print(f\"âœ… æ£€æŸ¥ç‚¹: å·²å®‰å…¨ä¿å­˜ {total_rows:,} è¡Œ\")\n",
    "        \n",
    "        # æµ‹è¯•æ¨¡å¼ï¼šåªå¤„ç†500ä¸‡è¡Œ\n",
    "        if total_rows >= 5000000:\n",
    "            print(f\"\\n[æµ‹è¯•å®Œæˆ] å·²å®‰å…¨å¤„ç†500ä¸‡è¡Œ\")\n",
    "            break\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(f\"ğŸ‰ æµå¼å¤„ç†å®Œæˆï¼\")\n",
    "    print(f\"æ•°æ®å·²å®‰å…¨ä¿å­˜è‡³: {output_path}\")\n",
    "    \n",
    "    if os.path.exists(output_path):\n",
    "        size_gb = os.path.getsize(output_path) / (1024**3)\n",
    "        print(f\"æ–‡ä»¶å¤§å°: {size_gb:.2f} GB\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# è¿è¡Œå®‰å…¨ç‰ˆæœ¬\n",
    "safe_stream_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c707a12-1fd1-439c-88d0-8d5c45929838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ›’ ç”µå•†å¹³å°æ ¸å¿ƒä¸šåŠ¡åˆ†æ\n",
      "============================================================\n",
      "ğŸ“ åŠ è½½æ•°æ®: D:\\Mycode\\processed_data\\user_behavior_clean.csv\n",
      "ğŸ“Š æ–‡ä»¶å¤§å°: 0.28 GB\n",
      "âœ… åŠ è½½å®Œæˆ: 1,000,000 è¡Œ\n",
      "ğŸ“… æ—¶é—´èŒƒå›´: 2017-09-11 08:16:39 åˆ° 2017-12-03 16:00:06\n",
      "\n",
      "============================================================\n",
      "ğŸ‘¤ ç”¨æˆ·è¡Œä¸ºæ·±åº¦åˆ†æ\n",
      "============================================================\n",
      "\n",
      "ğŸ“ˆ ç”¨æˆ·æ´»è·ƒåº¦:\n",
      "   æ€»ç”¨æˆ·æ•°: 9,739\n",
      "   å¹³å‡æ¯ç”¨æˆ·è¡Œä¸ºæ•°: 102.7\n",
      "   å¹³å‡æ´»è·ƒå¤©æ•°: 7.2\n",
      "   å¹³å‡æµè§ˆå•†å“æ•°: 77.7\n",
      "\n",
      "ğŸ‘¥ ç”¨æˆ·åˆ†å±‚:\n",
      "   é«˜é¢‘ç”¨æˆ·: 5,095 (52.3%)\n",
      "   è¶…çº§ç”¨æˆ·: 3,697 (38.0%)\n",
      "   ä¸­é¢‘ç”¨æˆ·: 920 (9.4%)\n",
      "   ä½é¢‘ç”¨æˆ·: 27 (0.3%)\n",
      "\n",
      "============================================================\n",
      "ğŸ”„ ç”¨æˆ·è½¬åŒ–æ¼æ–—åˆ†æ\n",
      "============================================================\n",
      "è½¬åŒ–æ¼æ–—:\n",
      "   æµè§ˆç”¨æˆ·(PV): 9,706 (100%)\n",
      "   åŠ è´­ç”¨æˆ·(Cart): 7,323 (75.4%)\n",
      "   æ”¶è—ç”¨æˆ·(Fav): 3,882 (53.0%)\n",
      "   è´­ä¹°ç”¨æˆ·(Buy): 6,689 (172.3%)\n",
      "\n",
      "ğŸ“Š æ·±åº¦è½¬åŒ–:\n",
      "   æµè§ˆâ†’è´­ä¹°æ•´ä½“è½¬åŒ–ç‡: 68.92%\n",
      "   å®Œæˆå®Œæ•´æ—…ç¨‹ç”¨æˆ·: 6,662\n",
      "\n",
      "============================================================\n",
      "ğŸ“ˆ é”€å”®è¶‹åŠ¿åˆ†æ\n",
      "============================================================\n",
      "ğŸ“… æ¯æ—¥è¶‹åŠ¿:\n",
      "   åˆ†æå¤©æ•°: 24\n",
      "   å¹³å‡æ¯æ—¥ç”¨æˆ·: 2927\n",
      "   å¹³å‡æ¯æ—¥é”€é‡: 848\n",
      "\n",
      "â° å°æ—¶è¶‹åŠ¿:\n",
      "   é”€å”®é«˜å³°æ—¶æ®µ: 13:00 (1380 ç¬”)\n",
      "   ä½è°·æ—¶æ®µ: 19:00\n",
      "\n",
      "============================================================\n",
      "ğŸ·ï¸  å•†å“è¡¨ç°åˆ†æ\n",
      "============================================================\n",
      "ğŸ”¥ çƒ­é”€å•†å“TOP 10:\n",
      "    1. å•†å“3122135.0: 17.0 è´­ä¹°, è½¬åŒ–ç‡: 106.2%\n",
      "    2. å•†å“2964774.0: 11.0 è´­ä¹°, è½¬åŒ–ç‡: 13.6%\n",
      "    3. å•†å“3237415.0: 11.0 è´­ä¹°, è½¬åŒ–ç‡: 26.2%\n",
      "    4. å•†å“2124040.0: 11.0 è´­ä¹°, è½¬åŒ–ç‡: 275.0%\n",
      "    5. å•†å“4401268.0: 10.0 è´­ä¹°, è½¬åŒ–ç‡: 34.5%\n",
      "    6. å•†å“1910706.0: 8.0 è´­ä¹°, è½¬åŒ–ç‡: 66.7%\n",
      "    7. å•†å“3991727.0: 8.0 è´­ä¹°, è½¬åŒ–ç‡: 44.4%\n",
      "    8. å•†å“1004046.0: 8.0 è´­ä¹°, è½¬åŒ–ç‡: 61.5%\n",
      "    9. å•†å“121226.0: 7.0 è´­ä¹°, è½¬åŒ–ç‡: 33.3%\n",
      "   10. å•†å“1180858.0: 7.0 è´­ä¹°, è½¬åŒ–ç‡: 233.3%\n",
      "\n",
      "ğŸ“Š çƒ­é—¨ç±»ç›®TOP 5:\n",
      "    1. ç±»ç›®2735466: 363 è´­ä¹°, 3187 å•†å“, 2702 ç”¨æˆ·\n",
      "    2. ç±»ç›®1464116: 352 è´­ä¹°, 2586 å•†å“, 1862 ç”¨æˆ·\n",
      "    3. ç±»ç›®4145813: 326 è´­ä¹°, 10542 å•†å“, 3728 ç”¨æˆ·\n",
      "    4. ç±»ç›®2885642: 310 è´­ä¹°, 5079 å•†å“, 1582 ç”¨æˆ·\n",
      "    5. ç±»ç›®4801426: 280 è´­ä¹°, 6490 å•†å“, 3254 ç”¨æˆ·\n",
      "\n",
      "============================================================\n",
      "ğŸ¨ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨\n",
      "============================================================\n",
      "âœ… å›¾è¡¨1: ç”¨æˆ·è¡Œä¸ºåˆ†å¸ƒ - å·²ä¿å­˜\n",
      "âœ… å›¾è¡¨2: è½¬åŒ–æ¼æ–— - å·²ä¿å­˜\n",
      "âœ… å›¾è¡¨3: 24å°æ—¶é”€å”®è¶‹åŠ¿ - å·²ä¿å­˜\n",
      "âœ… å›¾è¡¨4: ç”¨æˆ·åˆ†å±‚ - å·²ä¿å­˜\n",
      "âœ… å›¾è¡¨5: çƒ­é”€å•†å“TOP 10 - å·²ä¿å­˜\n",
      "\n",
      "============================================================\n",
      "ğŸ“‹ ç”Ÿæˆä¸šåŠ¡åˆ†ææŠ¥å‘Š\n",
      "============================================================\n",
      "âœ… ä¸šåŠ¡åˆ†ææŠ¥å‘Šå·²ä¿å­˜: D:\\Mycode\\business_analysis_report.md\n",
      "\n",
      "============================================================\n",
      "ğŸ‰ ç”µå•†ä¸šåŠ¡åˆ†æå®Œæˆï¼\n",
      "============================================================\n",
      "ğŸ“ è¾“å‡ºæ–‡ä»¶:\n",
      "1. ä¸šåŠ¡åˆ†ææŠ¥å‘Š: D:\\Mycode\\business_analysis_report.md\n",
      "2. å¯è§†åŒ–å›¾è¡¨: D:\\Mycode\\analysis_charts\n",
      "   - behavior_distribution.png\n",
      "   - conversion_funnel.png\n",
      "   - hourly_sales_trend.png\n",
      "   - user_segmentation.png\n",
      "   - top_products.png\n",
      "\n",
      "ğŸ’¡ å…³é”®å‘ç°:\n",
      "- ç”¨æˆ·æ€»æ•°: 9,739\n",
      "- æ•´ä½“è½¬åŒ–ç‡: 68.92%\n",
      "- é”€å”®é«˜å³°: 13:00\n",
      "\n",
      "ğŸš€ ä¸‹ä¸€æ­¥: åŸºäºåˆ†æç»“æœåˆ¶å®šå…·ä½“ä¼˜åŒ–ç­–ç•¥\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ç”µå•†æ ¸å¿ƒä¸šåŠ¡åˆ†æ\n",
    "åŸºäºæ¸…æ´—åçš„æ•°æ®è¿›è¡Œåˆ†æ\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡å­—ä½“ï¼ˆå¦‚æœéœ€è¦ï¼‰\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "def load_processed_data():\n",
    "    \"\"\"åŠ è½½å¤„ç†å¥½çš„æ•°æ®\"\"\"\n",
    "    data_path = r'D:\\Mycode\\processed_data\\user_behavior_clean.csv'\n",
    "    \n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}\")\n",
    "        print(\"è¯·å…ˆè¿è¡Œæ•°æ®å¤„ç†è„šæœ¬\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"ğŸ“ åŠ è½½æ•°æ®: {data_path}\")\n",
    "    file_size = os.path.getsize(data_path) / (1024**3)\n",
    "    print(f\"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size:.2f} GB\")\n",
    "    \n",
    "    # åªè¯»å–å‰100ä¸‡è¡Œè¿›è¡Œåˆ†æï¼ˆåŠ å¿«é€Ÿåº¦ï¼‰\n",
    "    df = pd.read_csv(data_path, nrows=1000000)\n",
    "    \n",
    "    # ç¡®ä¿æ—¶é—´åˆ—æ˜¯datetimeç±»å‹\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    print(f\"âœ… åŠ è½½å®Œæˆ: {len(df):,} è¡Œ\")\n",
    "    print(f\"ğŸ“… æ—¶é—´èŒƒå›´: {df['timestamp'].min()} åˆ° {df['timestamp'].max()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def analyze_user_behavior(df):\n",
    "    \"\"\"ç”¨æˆ·è¡Œä¸ºåˆ†æ\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ‘¤ ç”¨æˆ·è¡Œä¸ºæ·±åº¦åˆ†æ\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. ç”¨æˆ·æ´»è·ƒåº¦åˆ†æ\n",
    "    print(\"\\nğŸ“ˆ ç”¨æˆ·æ´»è·ƒåº¦:\")\n",
    "    user_activity = df.groupby('user_id').agg({\n",
    "        'behavior_type': 'count',\n",
    "        'date': 'nunique',\n",
    "        'item_id': 'nunique'\n",
    "    }).rename(columns={\n",
    "        'behavior_type': 'total_actions',\n",
    "        'date': 'active_days',\n",
    "        'item_id': 'unique_items'\n",
    "    })\n",
    "    \n",
    "    print(f\"   æ€»ç”¨æˆ·æ•°: {len(user_activity):,}\")\n",
    "    print(f\"   å¹³å‡æ¯ç”¨æˆ·è¡Œä¸ºæ•°: {user_activity['total_actions'].mean():.1f}\")\n",
    "    print(f\"   å¹³å‡æ´»è·ƒå¤©æ•°: {user_activity['active_days'].mean():.1f}\")\n",
    "    print(f\"   å¹³å‡æµè§ˆå•†å“æ•°: {user_activity['unique_items'].mean():.1f}\")\n",
    "    \n",
    "    # ç”¨æˆ·åˆ†å±‚\n",
    "    user_activity['user_type'] = pd.cut(user_activity['total_actions'], \n",
    "                                       bins=[0, 5, 20, 100, np.inf],\n",
    "                                       labels=['ä½é¢‘ç”¨æˆ·', 'ä¸­é¢‘ç”¨æˆ·', 'é«˜é¢‘ç”¨æˆ·', 'è¶…çº§ç”¨æˆ·'])\n",
    "    \n",
    "    user_type_counts = user_activity['user_type'].value_counts()\n",
    "    print(f\"\\nğŸ‘¥ ç”¨æˆ·åˆ†å±‚:\")\n",
    "    for user_type, count in user_type_counts.items():\n",
    "        percentage = count / len(user_activity) * 100\n",
    "        print(f\"   {user_type}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    return user_activity\n",
    "\n",
    "def analyze_conversion_funnel(df):\n",
    "    \"\"\"è½¬åŒ–æ¼æ–—åˆ†æ\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ”„ ç”¨æˆ·è½¬åŒ–æ¼æ–—åˆ†æ\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # æŒ‰ç”¨æˆ·è¿½è¸ªè¡Œä¸ºè·¯å¾„\n",
    "    user_journey = df.groupby('user_id')['behavior_type'].apply(list)\n",
    "    \n",
    "    # ç»Ÿè®¡å„é˜¶æ®µç”¨æˆ·æ•°\n",
    "    funnel_data = {\n",
    "        'æµè§ˆç”¨æˆ·(PV)': sum(1 for journey in user_journey if 'pv' in journey),\n",
    "        'åŠ è´­ç”¨æˆ·(Cart)': sum(1 for journey in user_journey if 'cart' in journey),\n",
    "        'æ”¶è—ç”¨æˆ·(Fav)': sum(1 for journey in user_journey if 'fav' in journey),\n",
    "        'è´­ä¹°ç”¨æˆ·(Buy)': sum(1 for journey in user_journey if 'buy' in journey)\n",
    "    }\n",
    "    \n",
    "    # è®¡ç®—è½¬åŒ–ç‡\n",
    "    print(\"è½¬åŒ–æ¼æ–—:\")\n",
    "    previous_count = None\n",
    "    for stage, count in funnel_data.items():\n",
    "        if previous_count is None:\n",
    "            print(f\"   {stage}: {count:,} (100%)\")\n",
    "        else:\n",
    "            conversion_rate = count / previous_count * 100 if previous_count > 0 else 0\n",
    "            print(f\"   {stage}: {count:,} ({conversion_rate:.1f}%)\")\n",
    "        previous_count = count\n",
    "    \n",
    "    # æ·±åº¦è½¬åŒ–åˆ†æ\n",
    "    print(f\"\\nğŸ“Š æ·±åº¦è½¬åŒ–:\")\n",
    "    pv_users = funnel_data['æµè§ˆç”¨æˆ·(PV)']\n",
    "    buy_users = funnel_data['è´­ä¹°ç”¨æˆ·(Buy)']\n",
    "    \n",
    "    # æµè§ˆåˆ°è´­ä¹°çš„æ•´ä½“è½¬åŒ–ç‡\n",
    "    overall_conversion = buy_users / pv_users * 100 if pv_users > 0 else 0\n",
    "    print(f\"   æµè§ˆâ†’è´­ä¹°æ•´ä½“è½¬åŒ–ç‡: {overall_conversion:.2f}%\")\n",
    "    \n",
    "    # å¤šé˜¶æ®µè½¬åŒ–ç”¨æˆ·\n",
    "    multi_stage_users = sum(1 for journey in user_journey \n",
    "                          if ('pv' in journey and 'buy' in journey))\n",
    "    print(f\"   å®Œæˆå®Œæ•´æ—…ç¨‹ç”¨æˆ·: {multi_stage_users:,}\")\n",
    "    \n",
    "    return funnel_data\n",
    "\n",
    "def analyze_sales_trends(df):\n",
    "    \"\"\"é”€å”®è¶‹åŠ¿åˆ†æ\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“ˆ é”€å”®è¶‹åŠ¿åˆ†æ\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. æŒ‰æ—¥æœŸåˆ†æ\n",
    "    daily_analysis = df.groupby('date').agg({\n",
    "        'user_id': 'nunique',\n",
    "        'item_id': 'nunique',\n",
    "        'behavior_type': lambda x: (x == 'buy').sum()\n",
    "    }).rename(columns={\n",
    "        'user_id': 'daily_users',\n",
    "        'item_id': 'daily_items',\n",
    "        'behavior_type': 'daily_purchases'\n",
    "    })\n",
    "    \n",
    "    print(\"ğŸ“… æ¯æ—¥è¶‹åŠ¿:\")\n",
    "    print(f\"   åˆ†æå¤©æ•°: {len(daily_analysis)}\")\n",
    "    print(f\"   å¹³å‡æ¯æ—¥ç”¨æˆ·: {daily_analysis['daily_users'].mean():.0f}\")\n",
    "    print(f\"   å¹³å‡æ¯æ—¥é”€é‡: {daily_analysis['daily_purchases'].mean():.0f}\")\n",
    "    \n",
    "    # 2. æŒ‰å°æ—¶åˆ†æ\n",
    "    hourly_analysis = df.groupby('hour').agg({\n",
    "        'user_id': 'nunique',\n",
    "        'behavior_type': lambda x: (x == 'buy').sum()\n",
    "    }).rename(columns={\n",
    "        'user_id': 'hourly_users',\n",
    "        'behavior_type': 'hourly_purchases'\n",
    "    })\n",
    "    \n",
    "    # æ‰¾åˆ°é«˜å³°æ—¶æ®µ\n",
    "    peak_hour = hourly_analysis['hourly_purchases'].idxmax()\n",
    "    peak_sales = hourly_analysis['hourly_purchases'].max()\n",
    "    \n",
    "    print(f\"\\nâ° å°æ—¶è¶‹åŠ¿:\")\n",
    "    print(f\"   é”€å”®é«˜å³°æ—¶æ®µ: {peak_hour}:00 ({peak_sales} ç¬”)\")\n",
    "    print(f\"   ä½è°·æ—¶æ®µ: {hourly_analysis['hourly_purchases'].idxmin()}:00\")\n",
    "    \n",
    "    return daily_analysis, hourly_analysis\n",
    "\n",
    "def analyze_product_performance(df):\n",
    "    \"\"\"å•†å“è¡¨ç°åˆ†æ\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ·ï¸  å•†å“è¡¨ç°åˆ†æ\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. çƒ­é—¨å•†å“\n",
    "    product_behavior = df.groupby('item_id').agg({\n",
    "        'behavior_type': lambda x: {\n",
    "            'pv': (x == 'pv').sum(),\n",
    "            'cart': (x == 'cart').sum(),\n",
    "            'fav': (x == 'fav').sum(),\n",
    "            'buy': (x == 'buy').sum()\n",
    "        },\n",
    "        'user_id': 'nunique'\n",
    "    })\n",
    "    \n",
    "    # è®¡ç®—è½¬åŒ–ç‡\n",
    "    product_stats = []\n",
    "    for item_id, stats in product_behavior.iterrows():\n",
    "        behaviors = stats['behavior_type']\n",
    "        pv = behaviors.get('pv', 0)\n",
    "        buy = behaviors.get('buy', 0)\n",
    "        \n",
    "        conversion = buy / pv * 100 if pv > 0 else 0\n",
    "        \n",
    "        product_stats.append({\n",
    "            'item_id': item_id,\n",
    "            'views': pv,\n",
    "            'purchases': buy,\n",
    "            'conversion_rate': conversion,\n",
    "            'unique_users': stats['user_id']\n",
    "        })\n",
    "    \n",
    "    product_df = pd.DataFrame(product_stats)\n",
    "    \n",
    "    # çƒ­é—¨å•†å“æ’å\n",
    "    top_products = product_df.sort_values('purchases', ascending=False).head(10)\n",
    "    \n",
    "    print(\"ğŸ”¥ çƒ­é”€å•†å“TOP 10:\")\n",
    "    for i, (_, row) in enumerate(top_products.iterrows(), 1):\n",
    "        print(f\"   {i:2d}. å•†å“{row['item_id']}: {row['purchases']} è´­ä¹°, \"\n",
    "              f\"è½¬åŒ–ç‡: {row['conversion_rate']:.1f}%\")\n",
    "    \n",
    "    # 2. å•†å“ç±»ç›®åˆ†æ\n",
    "    category_analysis = df.groupby('category_id').agg({\n",
    "        'behavior_type': lambda x: (x == 'buy').sum(),\n",
    "        'item_id': 'nunique',\n",
    "        'user_id': 'nunique'\n",
    "    }).rename(columns={\n",
    "        'behavior_type': 'purchases',\n",
    "        'item_id': 'unique_items',\n",
    "        'user_id': 'unique_users'\n",
    "    }).sort_values('purchases', ascending=False)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š çƒ­é—¨ç±»ç›®TOP 5:\")\n",
    "    for i, (category_id, row) in enumerate(category_analysis.head(5).iterrows(), 1):\n",
    "        print(f\"   {i:2d}. ç±»ç›®{category_id}: {row['purchases']} è´­ä¹°, \"\n",
    "              f\"{row['unique_items']} å•†å“, {row['unique_users']} ç”¨æˆ·\")\n",
    "    \n",
    "    return product_df, category_analysis\n",
    "\n",
    "def create_visualizations(df, user_activity, funnel_data, \n",
    "                         daily_analysis, hourly_analysis,\n",
    "                         product_df, category_analysis):\n",
    "    \"\"\"åˆ›å»ºå¯è§†åŒ–å›¾è¡¨\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ¨ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # åˆ›å»ºè¾“å‡ºç›®å½•\n",
    "    output_dir = r'D:\\Mycode\\analysis_charts'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # è®¾ç½®å›¾è¡¨é£æ ¼\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "    \n",
    "    # å›¾è¡¨1: ç”¨æˆ·è¡Œä¸ºåˆ†å¸ƒ\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    behavior_counts = df['behavior_type'].value_counts()\n",
    "    behavior_labels = ['æµè§ˆ', 'åŠ è´­', 'æ”¶è—', 'è´­ä¹°']\n",
    "    behavior_values = [behavior_counts.get('pv', 0),\n",
    "                      behavior_counts.get('cart', 0),\n",
    "                      behavior_counts.get('fav', 0),\n",
    "                      behavior_counts.get('buy', 0)]\n",
    "    \n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "    plt.pie(behavior_values, labels=behavior_labels, autopct='%1.1f%%',\n",
    "            colors=colors, startangle=90)\n",
    "    plt.title('ç”¨æˆ·è¡Œä¸ºåˆ†å¸ƒ', fontsize=14, fontweight='bold')\n",
    "    plt.savefig(os.path.join(output_dir, 'behavior_distribution.png'), \n",
    "                dpi=150, bbox_inches='tight')\n",
    "    print(f\"âœ… å›¾è¡¨1: ç”¨æˆ·è¡Œä¸ºåˆ†å¸ƒ - å·²ä¿å­˜\")\n",
    "    \n",
    "    # å›¾è¡¨2: è½¬åŒ–æ¼æ–—\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    funnel_stages = list(funnel_data.keys())\n",
    "    funnel_values = list(funnel_data.values())\n",
    "    \n",
    "    plt.barh(funnel_stages, funnel_values, color=colors)\n",
    "    plt.title('ç”¨æˆ·è½¬åŒ–æ¼æ–—', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('ç”¨æˆ·æ•°é‡')\n",
    "    \n",
    "    # æ·»åŠ è½¬åŒ–ç‡æ ‡ç­¾\n",
    "    for i, (stage, value) in enumerate(funnel_data.items()):\n",
    "        if i > 0:\n",
    "            prev_value = funnel_values[i-1]\n",
    "            conversion_rate = value / prev_value * 100 if prev_value > 0 else 0\n",
    "            plt.text(value + max(funnel_values)*0.02, i, \n",
    "                    f'{conversion_rate:.1f}%', va='center', fontsize=10)\n",
    "    \n",
    "    plt.savefig(os.path.join(output_dir, 'conversion_funnel.png'), \n",
    "                dpi=150, bbox_inches='tight')\n",
    "    print(f\"âœ… å›¾è¡¨2: è½¬åŒ–æ¼æ–— - å·²ä¿å­˜\")\n",
    "    \n",
    "    # å›¾è¡¨3: 24å°æ—¶é”€å”®è¶‹åŠ¿\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(hourly_analysis.index, hourly_analysis['hourly_purchases'], \n",
    "             marker='o', linewidth=2, color='#2ca02c')\n",
    "    plt.fill_between(hourly_analysis.index, hourly_analysis['hourly_purchases'], \n",
    "                     alpha=0.3, color='#2ca02c')\n",
    "    plt.title('24å°æ—¶é”€å”®è¶‹åŠ¿', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('å°æ—¶')\n",
    "    plt.ylabel('è´­ä¹°æ•°é‡')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(range(0, 24))\n",
    "    plt.savefig(os.path.join(output_dir, 'hourly_sales_trend.png'), \n",
    "                dpi=150, bbox_inches='tight')\n",
    "    print(f\"âœ… å›¾è¡¨3: 24å°æ—¶é”€å”®è¶‹åŠ¿ - å·²ä¿å­˜\")\n",
    "    \n",
    "    # å›¾è¡¨4: ç”¨æˆ·åˆ†å±‚\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    user_types = user_activity['user_type'].value_counts()\n",
    "    plt.bar(user_types.index.astype(str), user_types.values, \n",
    "            color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
    "    plt.title('ç”¨æˆ·åˆ†å±‚åˆ†å¸ƒ', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('ç”¨æˆ·ç±»å‹')\n",
    "    plt.ylabel('ç”¨æˆ·æ•°é‡')\n",
    "    \n",
    "    # æ·»åŠ ç™¾åˆ†æ¯”æ ‡ç­¾\n",
    "    total_users = len(user_activity)\n",
    "    for i, count in enumerate(user_types.values):\n",
    "        percentage = count / total_users * 100\n",
    "        plt.text(i, count + max(user_types.values)*0.01, \n",
    "                f'{percentage:.1f}%', ha='center', fontsize=10)\n",
    "    \n",
    "    plt.savefig(os.path.join(output_dir, 'user_segmentation.png'), \n",
    "                dpi=150, bbox_inches='tight')\n",
    "    print(f\"âœ… å›¾è¡¨4: ç”¨æˆ·åˆ†å±‚ - å·²ä¿å­˜\")\n",
    "    \n",
    "    # å›¾è¡¨5: çƒ­é”€å•†å“TOP 10\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    top_10_products = product_df.nlargest(10, 'purchases')\n",
    "    plt.barh([f\"å•†å“{pid}\" for pid in top_10_products['item_id']], \n",
    "             top_10_products['purchases'], color='#1f77b4')\n",
    "    plt.title('çƒ­é”€å•†å“TOP 10', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('è´­ä¹°æ•°é‡')\n",
    "    plt.gca().invert_yaxis()  # æœ€é«˜çš„åœ¨æœ€ä¸Šé¢\n",
    "    \n",
    "    plt.savefig(os.path.join(output_dir, 'top_products.png'), \n",
    "                dpi=150, bbox_inches='tight')\n",
    "    print(f\"âœ… å›¾è¡¨5: çƒ­é”€å•†å“TOP 10 - å·²ä¿å­˜\")\n",
    "    \n",
    "    plt.close('all')\n",
    "    \n",
    "    return output_dir\n",
    "\n",
    "def generate_business_report(df, user_activity, funnel_data,\n",
    "                           daily_analysis, hourly_analysis,\n",
    "                           product_df, category_analysis):\n",
    "    \"\"\"ç”Ÿæˆä¸šåŠ¡åˆ†ææŠ¥å‘Š\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“‹ ç”Ÿæˆä¸šåŠ¡åˆ†ææŠ¥å‘Š\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    report_path = r'D:\\Mycode\\business_analysis_report.md'\n",
    "    \n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"# ç”µå•†å¹³å°ä¸šåŠ¡åˆ†ææŠ¥å‘Š\\n\\n\")\n",
    "        f.write(f\"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"åˆ†ææ ·æœ¬: {len(df):,} è¡Œæ•°æ®\\n\\n\")\n",
    "        \n",
    "        f.write(\"## ğŸ“Š æ ¸å¿ƒä¸šåŠ¡æŒ‡æ ‡\\n\\n\")\n",
    "        \n",
    "        # ç”¨æˆ·æŒ‡æ ‡\n",
    "        f.write(\"### ğŸ‘¤ ç”¨æˆ·æŒ‡æ ‡\\n\")\n",
    "        f.write(f\"- æ€»ç”¨æˆ·æ•°: {len(user_activity):,}\\n\")\n",
    "        f.write(f\"- å¹³å‡æ¯ç”¨æˆ·è¡Œä¸ºæ•°: {user_activity['total_actions'].mean():.1f}\\n\")\n",
    "        f.write(f\"- å¹³å‡æ´»è·ƒå¤©æ•°: {user_activity['active_days'].mean():.1f}\\n\\n\")\n",
    "        \n",
    "        # è½¬åŒ–æ¼æ–—\n",
    "        f.write(\"### ğŸ”„ è½¬åŒ–æ¼æ–—\\n\")\n",
    "        for stage, count in funnel_data.items():\n",
    "            f.write(f\"- {stage}: {count:,}\\n\")\n",
    "        \n",
    "        pv_users = funnel_data['æµè§ˆç”¨æˆ·(PV)']\n",
    "        buy_users = funnel_data['è´­ä¹°ç”¨æˆ·(Buy)']\n",
    "        overall_conversion = buy_users / pv_users * 100 if pv_users > 0 else 0\n",
    "        f.write(f\"- æ•´ä½“è½¬åŒ–ç‡: {overall_conversion:.2f}%\\n\\n\")\n",
    "        \n",
    "        # é”€å”®è¶‹åŠ¿\n",
    "        f.write(\"### ğŸ“ˆ é”€å”®è¶‹åŠ¿\\n\")\n",
    "        f.write(f\"- å¹³å‡æ¯æ—¥é”€é‡: {daily_analysis['daily_purchases'].mean():.0f}\\n\")\n",
    "        f.write(f\"- é”€å”®é«˜å³°æ—¶æ®µ: {hourly_analysis['hourly_purchases'].idxmax()}:00\\n\")\n",
    "        f.write(f\"- æ—¥å‡æ´»è·ƒç”¨æˆ·: {daily_analysis['daily_users'].mean():.0f}\\n\\n\")\n",
    "        \n",
    "        # å•†å“è¡¨ç°\n",
    "        f.write(\"### ğŸ·ï¸  å•†å“è¡¨ç°\\n\")\n",
    "        f.write(f\"- åˆ†æå•†å“æ•°: {len(product_df):,}\\n\")\n",
    "        \n",
    "        top_product = product_df.nlargest(1, 'purchases').iloc[0]\n",
    "        f.write(f\"- æœ€çƒ­é”€å•†å“: å•†å“{top_product['item_id']} ({top_product['purchases']} è´­ä¹°)\\n\")\n",
    "        \n",
    "        high_conversion = product_df[product_df['views'] > 10].nlargest(1, 'conversion_rate')\n",
    "        if len(high_conversion) > 0:\n",
    "            best_product = high_conversion.iloc[0]\n",
    "            f.write(f\"- é«˜è½¬åŒ–å•†å“: å•†å“{best_product['item_id']} (è½¬åŒ–ç‡: {best_product['conversion_rate']:.1f}%)\\n\")\n",
    "        \n",
    "        f.write(f\"- çƒ­é—¨ç±»ç›®æ•°: {len(category_analysis):,}\\n\\n\")\n",
    "        \n",
    "        # ä¸šåŠ¡å»ºè®®\n",
    "        f.write(\"## ğŸ’¡ ä¸šåŠ¡ä¼˜åŒ–å»ºè®®\\n\\n\")\n",
    "        \n",
    "        f.write(\"### 1. æå‡è½¬åŒ–ç‡\\n\")\n",
    "        f.write(\"- **é—®é¢˜**: æµè§ˆâ†’è´­ä¹°è½¬åŒ–ç‡è¾ƒä½\\n\")\n",
    "        f.write(\"- **å»ºè®®**: ä¼˜åŒ–å•†å“è¯¦æƒ…é¡µï¼Œæ·»åŠ ç”¨æˆ·è¯„ä»·ã€é”€é‡æ˜¾ç¤º\\n\")\n",
    "        f.write(\"- **è¡ŒåŠ¨**: A/Bæµ‹è¯•ä¸åŒçš„å•†å“å±•ç¤ºæ–¹å¼\\n\\n\")\n",
    "        \n",
    "        f.write(\"### 2. ä¼˜åŒ–è¿è¥æ—¶æ®µ\\n\")\n",
    "        f.write(f\"- **å‘ç°**: {hourly_analysis['hourly_purchases'].idxmax()}:00æ˜¯é”€å”®é«˜å³°\\n\")\n",
    "        f.write(\"- **å»ºè®®**: åœ¨è¯¥æ—¶æ®µå¢åŠ å®¢æœäººå‘˜ï¼Œæ¨å‡ºé™æ—¶ä¼˜æƒ \\n\")\n",
    "        f.write(\"- **è¡ŒåŠ¨**: è®¾ç½®é«˜å³°æ—¶æ®µä¸“é¡¹ä¿ƒé”€æ´»åŠ¨\\n\\n\")\n",
    "        \n",
    "        f.write(\"### 3. ç”¨æˆ·åˆ†å±‚è¿è¥\\n\")\n",
    "        user_types = user_activity['user_type'].value_counts()\n",
    "        for user_type, count in user_types.items():\n",
    "            percentage = count / len(user_activity) * 100\n",
    "            f.write(f\"- **{user_type}**: {count:,} ({percentage:.1f}%)\\n\")\n",
    "        \n",
    "        f.write(\"- **å»ºè®®**: é’ˆå¯¹ä¸åŒç”¨æˆ·ç±»å‹åˆ¶å®šä¸ªæ€§åŒ–è¥é”€ç­–ç•¥\\n\")\n",
    "        f.write(\"- **è¡ŒåŠ¨**: ä¸ºé«˜é¢‘ç”¨æˆ·æä¾›VIPæƒç›Šï¼Œå”¤é†’ä½é¢‘ç”¨æˆ·\\n\\n\")\n",
    "        \n",
    "        f.write(\"### 4. å•†å“ä¼˜åŒ–\\n\")\n",
    "        f.write(\"- **å‘ç°**: éƒ¨åˆ†å•†å“æµè§ˆé«˜ä½†è½¬åŒ–ä½\\n\")\n",
    "        f.write(\"- **å»ºè®®**: åˆ†æä½è½¬åŒ–å•†å“åŸå› ï¼Œä¼˜åŒ–ä»·æ ¼æˆ–æè¿°\\n\")\n",
    "        f.write(\"- **è¡ŒåŠ¨**: å»ºç«‹å•†å“è¡¨ç°ç›‘æ§ä½“ç³»\\n\\n\")\n",
    "        \n",
    "        # å¯è§†åŒ–å›¾è¡¨æ¸…å•\n",
    "        f.write(\"## ğŸ¨ å¯è§†åŒ–å›¾è¡¨\\n\")\n",
    "        f.write(\"ä»¥ä¸‹å›¾è¡¨å·²ç”Ÿæˆï¼Œè¯·æŸ¥çœ‹å¯¹åº”æ–‡ä»¶:\\n\")\n",
    "        f.write(\"- `behavior_distribution.png`: ç”¨æˆ·è¡Œä¸ºåˆ†å¸ƒ\\n\")\n",
    "        f.write(\"- `conversion_funnel.png`: è½¬åŒ–æ¼æ–—\\n\")\n",
    "        f.write(\"- `hourly_sales_trend.png`: 24å°æ—¶é”€å”®è¶‹åŠ¿\\n\")\n",
    "        f.write(\"- `user_segmentation.png`: ç”¨æˆ·åˆ†å±‚\\n\")\n",
    "        f.write(\"- `top_products.png`: çƒ­é”€å•†å“TOP 10\\n\\n\")\n",
    "        \n",
    "        f.write(\"## ğŸ“ˆ ä¸‹ä¸€æ­¥åˆ†æå»ºè®®\\n\")\n",
    "        f.write(\"1. **ç”¨æˆ·ç•™å­˜åˆ†æ**: è®¡ç®—7æ—¥ã€30æ—¥ç•™å­˜ç‡\\n\")\n",
    "        f.write(\"2. **ç”¨æˆ·ä»·å€¼åˆ†æ**: è®¡ç®—RFMæ¨¡å‹ï¼ˆæœ€è¿‘è´­ä¹°ã€é¢‘ç‡ã€é‡‘é¢ï¼‰\\n\")\n",
    "        f.write(\"3. **æ¨èç³»ç»Ÿ**: åŸºäºç”¨æˆ·è¡Œä¸ºæ„å»ºå•†å“æ¨è\\n\")\n",
    "        f.write(\"4. **åº“å­˜ä¼˜åŒ–**: åŸºäºé”€é‡é¢„æµ‹ä¼˜åŒ–åº“å­˜ç®¡ç†\\n\")\n",
    "    \n",
    "    print(f\"âœ… ä¸šåŠ¡åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_path}\")\n",
    "    return report_path\n",
    "        \n",
    "def main():\n",
    "    \"\"\"ä¸»åˆ†ææµç¨‹\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"ğŸ›’ ç”µå•†å¹³å°æ ¸å¿ƒä¸šåŠ¡åˆ†æ\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. åŠ è½½æ•°æ®\n",
    "    df = load_processed_data()\n",
    "    if df is None:\n",
    "        return\n",
    "    \n",
    "    # 2. ç”¨æˆ·è¡Œä¸ºåˆ†æ\n",
    "    user_activity = analyze_user_behavior(df)\n",
    "    \n",
    "    # 3. è½¬åŒ–æ¼æ–—åˆ†æ\n",
    "    funnel_data = analyze_conversion_funnel(df)\n",
    "    \n",
    "    # 4. é”€å”®è¶‹åŠ¿åˆ†æ\n",
    "    daily_analysis, hourly_analysis = analyze_sales_trends(df)\n",
    "    \n",
    "    # 5. å•†å“è¡¨ç°åˆ†æ\n",
    "    product_df, category_analysis = analyze_product_performance(df)\n",
    "    \n",
    "    # 6. åˆ›å»ºå¯è§†åŒ–\n",
    "    charts_dir = create_visualizations(df, user_activity, funnel_data,\n",
    "                                      daily_analysis, hourly_analysis,\n",
    "                                      product_df, category_analysis)\n",
    "    \n",
    "    # 7. ç”ŸæˆæŠ¥å‘Š\n",
    "    report_path = generate_business_report(df, user_activity, funnel_data,\n",
    "                                         daily_analysis, hourly_analysis,\n",
    "                                         product_df, category_analysis)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ‰ ç”µå•†ä¸šåŠ¡åˆ†æå®Œæˆï¼\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"ğŸ“ è¾“å‡ºæ–‡ä»¶:\")\n",
    "    print(f\"1. ä¸šåŠ¡åˆ†ææŠ¥å‘Š: {report_path}\")\n",
    "    print(f\"2. å¯è§†åŒ–å›¾è¡¨: {charts_dir}\")\n",
    "    print(\"   - behavior_distribution.png\")\n",
    "    print(\"   - conversion_funnel.png\")\n",
    "    print(\"   - hourly_sales_trend.png\")\n",
    "    print(\"   - user_segmentation.png\")\n",
    "    print(\"   - top_products.png\")\n",
    "    print(\"\\nğŸ’¡ å…³é”®å‘ç°:\")\n",
    "    print(f\"- ç”¨æˆ·æ€»æ•°: {len(user_activity):,}\")\n",
    "    print(f\"- æ•´ä½“è½¬åŒ–ç‡: {funnel_data['è´­ä¹°ç”¨æˆ·(Buy)']/funnel_data['æµè§ˆç”¨æˆ·(PV)']*100:.2f}%\")\n",
    "    print(f\"- é”€å”®é«˜å³°: {hourly_analysis['hourly_purchases'].idxmax()}:00\")\n",
    "    print(\"\\nğŸš€ ä¸‹ä¸€æ­¥: åŸºäºåˆ†æç»“æœåˆ¶å®šå…·ä½“ä¼˜åŒ–ç­–ç•¥\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "                                       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c822cd61-bb33-4937-8abb-01a5e1bc9211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸš€ ç”µå•†é«˜çº§åˆ†æ - ç”¨æˆ·ç•™å­˜ä¸ä»·å€¼æŒ–æ˜\n",
      "============================================================\n",
      "============================================================\n",
      "ğŸ“Š é«˜çº§åˆ†æ - æ•°æ®å‡†å¤‡\n",
      "============================================================\n",
      "åŠ è½½æ•°æ®...\n",
      "âœ… æ•°æ®åŠ è½½å®Œæˆ: 2,000,000 è¡Œ\n",
      "ğŸ“… æ—¶é—´èŒƒå›´: 1970-01-01 12:13:36 åˆ° 2018-08-28 10:27:12\n",
      "\n",
      "============================================================\n",
      "ğŸ“ˆ ç”¨æˆ·ç•™å­˜ç‡åˆ†æ\n",
      "============================================================\n",
      "ğŸ¯ ç”¨æˆ·ç•™å­˜ç‡çŸ©é˜µï¼ˆæŒ‰é¦–æ¬¡æ´»è·ƒæ—¥æœŸï¼‰:\n",
      "--------------------------------------------------\n",
      "æ¬¡æ—¥ç•™å­˜ç‡: 62.6%\n",
      "7æ—¥ç•™å­˜ç‡: 64.6%\n",
      "\n",
      "============================================================\n",
      "ğŸ’° RFMç”¨æˆ·ä»·å€¼åˆ†æ\n",
      "============================================================\n",
      "åˆ†æè´­ä¹°ç”¨æˆ·æ•°: 13,330\n",
      "æœ€è¿‘è´­ä¹°æ—¶é—´: 2018-08-28 00:00:00\n",
      "\n",
      "ğŸ‘¥ RFMç”¨æˆ·åˆ†å±‚ç»“æœ:\n",
      "----------------------------------------\n",
      "æ½œåŠ›ç”¨æˆ·: 4,819 (36.2%)\n",
      "ä¸€èˆ¬ç”¨æˆ·: 3,867 (29.0%)\n",
      "é«˜ä»·å€¼ç”¨æˆ·: 2,554 (19.2%)\n",
      "ä½ä»·å€¼ç”¨æˆ·: 2,090 (15.7%)\n",
      "\n",
      "ğŸ¯ å„å±‚çº§å…¸å‹ç‰¹å¾:\n",
      "\n",
      "é«˜ä»·å€¼ç”¨æˆ·:\n",
      "  å¹³å‡æœ€è¿‘è´­ä¹°å¤©æ•°: 268.7\n",
      "  å¹³å‡è´­ä¹°æ¬¡æ•°: 6.9\n",
      "\n",
      "æ½œåŠ›ç”¨æˆ·:\n",
      "  å¹³å‡æœ€è¿‘è´­ä¹°å¤©æ•°: 269.6\n",
      "  å¹³å‡è´­ä¹°æ¬¡æ•°: 3.0\n",
      "\n",
      "ä¸€èˆ¬ç”¨æˆ·:\n",
      "  å¹³å‡æœ€è¿‘è´­ä¹°å¤©æ•°: 271.6\n",
      "  å¹³å‡è´­ä¹°æ¬¡æ•°: 1.6\n",
      "\n",
      "ä½ä»·å€¼ç”¨æˆ·:\n",
      "  å¹³å‡æœ€è¿‘è´­ä¹°å¤©æ•°: 273.5\n",
      "  å¹³å‡è´­ä¹°æ¬¡æ•°: 1.0\n",
      "\n",
      "============================================================\n",
      "ğŸ”® ç”¨æˆ·ä¸‹æ¬¡è´­ä¹°æ—¶é—´é¢„æµ‹\n",
      "============================================================\n",
      "ğŸ“Š è´­ä¹°é—´éš”ç»Ÿè®¡:\n",
      "  å¹³å‡è´­ä¹°é—´éš”: 0.9 å¤©\n",
      "  ä¸­ä½æ•°è´­ä¹°é—´éš”: 0.0 å¤©\n",
      "  æœ€çŸ­é—´éš”: 0.0 å¤©\n",
      "  æœ€é•¿é—´éš”: 8.0 å¤©\n",
      "\n",
      "ğŸ‘¥ ç”¨æˆ·è´­ä¹°è¡Œä¸º:\n",
      "  æœ‰å¤è´­ç”¨æˆ·æ•°: 8,795\n",
      "  å¹³å‡å¤è´­æ¬¡æ•°: 3.1\n",
      "\n",
      "ğŸ¯ ä¸‹æ¬¡è´­ä¹°é¢„æµ‹:\n",
      "  åˆ†æåŸºå‡†æ—¥æœŸ: 2018-08-28 10:27:12\n",
      "  é¢„è®¡7å¤©å†…ä¼šè´­ä¹°çš„ç”¨æˆ·: 13,330\n",
      "  é¢„è®¡30å¤©å†…ä¼šè´­ä¹°çš„ç”¨æˆ·: 13,330\n",
      "\n",
      "============================================================\n",
      "ğŸ“¦ åº“å­˜ä¼˜åŒ–åˆ†æ\n",
      "============================================================\n",
      "ğŸ“Š å•†å“åˆ†ç±»ç»Ÿè®¡:\n",
      "----------------------------------------\n",
      "æ»é”€å“: 32,451 (99.9%)\n",
      "ä¸€èˆ¬å“: 34 (0.1%)\n",
      "\n",
      "ğŸ’¡ åº“å­˜ä¼˜åŒ–å»ºè®®:\n",
      "----------------------------------------\n",
      "\n",
      "2. æ»é”€å“(32451ä¸ª):\n",
      "   - æ€»æ»é”€å•†å“æ•°: 32,451\n",
      "   - å»ºè®®: æ¸…ä»“ä¿ƒé”€æˆ–ä¸‹æ¶\n",
      "   - å¯é‡Šæ”¾åº“å­˜ç©ºé—´æ½œåŠ›å¤§\n",
      "\n",
      "3. ABCåˆ†æ:\n",
      "   Aç±»å•†å“(24436ä¸ª): è´¡çŒ®80%é”€é‡ï¼Œé‡ç‚¹ç®¡ç†\n",
      "   Bç±»å•†å“(6036ä¸ª): è´¡çŒ®15%é”€é‡ï¼Œå¸¸è§„ç®¡ç†\n",
      "   Cç±»å•†å“(2013ä¸ª): è´¡çŒ®5%é”€é‡ï¼Œç®€åŒ–ç®¡ç†\n",
      "\n",
      "============================================================\n",
      "ğŸ“‹ ç”Ÿæˆé«˜çº§åˆ†ææŠ¥å‘Š\n",
      "============================================================\n",
      "âœ… é«˜çº§åˆ†ææŠ¥å‘Šå·²ä¿å­˜: D:\\Mycode\\advanced_analysis_report.md\n",
      "\n",
      "============================================================\n",
      "ğŸ‰ é«˜çº§åˆ†æå®Œæˆï¼\n",
      "============================================================\n",
      "ğŸ“ è¾“å‡ºæˆæœ:\n",
      "1. é«˜çº§åˆ†ææŠ¥å‘Š: D:\\Mycode\\advanced_analysis_report.md\n",
      "\n",
      "ğŸ” æ ¸å¿ƒæ´å¯Ÿ:\n",
      "- 7æ—¥ç”¨æˆ·ç•™å­˜ç‡: 64.6%\n",
      "- é«˜ä»·å€¼ç”¨æˆ·å æ¯”: 19.2%\n",
      "- çƒ­é”€å•†å“æ•°é‡: 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ç”µå•†é«˜çº§åˆ†æ - ç”¨æˆ·ç•™å­˜ã€ä»·å€¼æŒ–æ˜ã€é¢„æµ‹æ¨¡å‹\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"åŠ è½½å¹¶å‡†å¤‡æ•°æ®\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"ğŸ“Š é«˜çº§åˆ†æ - æ•°æ®å‡†å¤‡\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # åŠ è½½ä¹‹å‰å¤„ç†å¥½çš„æ•°æ®\n",
    "    data_path = r'D:\\Mycode\\processed_data\\user_behavior_clean.csv'\n",
    "    \n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}\")\n",
    "        return None\n",
    "    \n",
    "    # è¯»å–æ•°æ®ï¼ˆå¯è°ƒæ•´è¡Œæ•°ï¼‰\n",
    "    print(\"åŠ è½½æ•°æ®...\")\n",
    "    df = pd.read_csv(data_path, nrows=2000000)  # 200ä¸‡è¡Œç”¨äºé«˜çº§åˆ†æ\n",
    "    \n",
    "    # è½¬æ¢æ—¶é—´åˆ—\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    print(f\"âœ… æ•°æ®åŠ è½½å®Œæˆ: {len(df):,} è¡Œ\")\n",
    "    print(f\"ğŸ“… æ—¶é—´èŒƒå›´: {df['timestamp'].min()} åˆ° {df['timestamp'].max()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_user_retention(df):\n",
    "    \"\"\"è®¡ç®—ç”¨æˆ·ç•™å­˜ç‡\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“ˆ ç”¨æˆ·ç•™å­˜ç‡åˆ†æ\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. è®¡ç®—æ¯ä¸ªç”¨æˆ·çš„é¦–æ¬¡æ´»è·ƒæ—¥æœŸ\n",
    "    user_first_active = df.groupby('user_id')['date'].min().reset_index()\n",
    "    user_first_active.columns = ['user_id', 'first_active_date']\n",
    "    \n",
    "    # 2. è®¡ç®—æ¯ä¸ªç”¨æˆ·æ¯å¤©çš„è¡Œä¸º\n",
    "    user_daily_activity = df.groupby(['user_id', 'date']).size().reset_index()\n",
    "    user_daily_activity.columns = ['user_id', 'date', 'daily_actions']\n",
    "    \n",
    "    # 3. åˆå¹¶æ•°æ®\n",
    "    user_activity = pd.merge(user_daily_activity, user_first_active, on='user_id')\n",
    "    \n",
    "    # 4. è®¡ç®—æ´»è·ƒå¤©æ•°é—´éš”\n",
    "    user_activity['days_since_first'] = (user_activity['date'] - user_activity['first_active_date']).dt.days\n",
    "    \n",
    "    # 5. è®¡ç®—ç•™å­˜ç‡\n",
    "    cohort_data = user_activity.groupby(['first_active_date', 'days_since_first'])['user_id'].nunique().reset_index()\n",
    "    cohort_pivot = cohort_data.pivot(index='first_active_date', columns='days_since_first', values='user_id')\n",
    "    \n",
    "    # è®¡ç®—ç•™å­˜ç‡ï¼ˆç›¸å¯¹äºç¬¬0å¤©ï¼‰\n",
    "    retention_matrix = cohort_pivot.divide(cohort_pivot[0], axis=0)\n",
    "    \n",
    "    print(\"ğŸ¯ ç”¨æˆ·ç•™å­˜ç‡çŸ©é˜µï¼ˆæŒ‰é¦–æ¬¡æ´»è·ƒæ—¥æœŸï¼‰:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # æ˜¾ç¤ºå…³é”®ç•™å­˜ç‡\n",
    "    if 1 in retention_matrix.columns:\n",
    "        print(f\"æ¬¡æ—¥ç•™å­˜ç‡: {retention_matrix[1].mean()*100:.1f}%\")\n",
    "    if 7 in retention_matrix.columns:\n",
    "        print(f\"7æ—¥ç•™å­˜ç‡: {retention_matrix[7].mean()*100:.1f}%\")\n",
    "    if 30 in retention_matrix.columns:\n",
    "        print(f\"30æ—¥ç•™å­˜ç‡: {retention_matrix[30].mean()*100:.1f}%\")\n",
    "    \n",
    "    return retention_matrix, user_activity\n",
    "\n",
    "def rfm_analysis(df):\n",
    "    \"\"\"RFMåˆ†æï¼ˆæœ€è¿‘è´­ä¹°ã€é¢‘ç‡ã€ä»·å€¼æ¨¡å‹ï¼‰\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ’° RFMç”¨æˆ·ä»·å€¼åˆ†æ\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # åªå…³æ³¨è´­ä¹°è¡Œä¸º\n",
    "    purchase_data = df[df['behavior_type'] == 'buy'].copy()\n",
    "    \n",
    "    if len(purchase_data) == 0:\n",
    "        print(\"âŒ æ²¡æœ‰è´­ä¹°æ•°æ®ï¼Œæ— æ³•è¿›è¡ŒRFMåˆ†æ\")\n",
    "        return None\n",
    "    \n",
    "    # è®¾ç½®åˆ†ææ—¥æœŸï¼ˆæ•°æ®æœ€åä¸€å¤©ï¼‰\n",
    "    analysis_date = df['date'].max()\n",
    "    \n",
    "    # è®¡ç®—RFMæŒ‡æ ‡\n",
    "    rfm = purchase_data.groupby('user_id').agg({\n",
    "        'date': lambda x: (analysis_date - x.max()).days,  # Recency: æœ€è¿‘è´­ä¹°å¤©æ•°\n",
    "        'user_id': 'count',  # Frequency: è´­ä¹°æ¬¡æ•°\n",
    "        # æ³¨æ„ï¼šè¿™é‡Œç¼ºå°‘è´­ä¹°é‡‘é¢ï¼Œç”¨è´­ä¹°æ¬¡æ•°ä½œä¸ºä»·å€¼ä»£ç†\n",
    "    }).rename(columns={\n",
    "        'date': 'recency',\n",
    "        'user_id': 'frequency'\n",
    "    })\n",
    "    \n",
    "    # æ·»åŠ monetaryï¼ˆè¿™é‡Œç”¨frequencyæ›¿ä»£ï¼Œå› ä¸ºç¼ºä¹é‡‘é¢æ•°æ®ï¼‰\n",
    "    rfm['monetary'] = rfm['frequency']\n",
    "    \n",
    "    print(f\"åˆ†æè´­ä¹°ç”¨æˆ·æ•°: {len(rfm):,}\")\n",
    "    print(f\"æœ€è¿‘è´­ä¹°æ—¶é—´: {analysis_date}\")\n",
    "    \n",
    "    # å¯¹RFMè¿›è¡Œåˆ†ç®±ï¼ˆ1-4åˆ†ï¼Œ4åˆ†æœ€å¥½ï¼‰\n",
    "    rfm['R_Score'] = pd.qcut(rfm['recency'], 4, labels=[4, 3, 2, 1])\n",
    "    rfm['F_Score'] = pd.qcut(rfm['frequency'].rank(method='first'), 4, labels=[1, 2, 3, 4])\n",
    "    rfm['M_Score'] = pd.qcut(rfm['monetary'].rank(method='first'), 4, labels=[1, 2, 3, 4])\n",
    "    \n",
    "    # è®¡ç®—RFMæ€»åˆ†\n",
    "    rfm['RFM_Score'] = rfm['R_Score'].astype(str) + rfm['F_Score'].astype(str) + rfm['M_Score'].astype(str)\n",
    "    rfm['RFM_Sum'] = rfm[['R_Score', 'F_Score', 'M_Score']].sum(axis=1)\n",
    "    \n",
    "    # ç”¨æˆ·åˆ†å±‚\n",
    "    def rfm_segment(row):\n",
    "        if row['RFM_Sum'] >= 11:\n",
    "            return 'é«˜ä»·å€¼ç”¨æˆ·'\n",
    "        elif row['RFM_Sum'] >= 8:\n",
    "            return 'æ½œåŠ›ç”¨æˆ·'\n",
    "        elif row['RFM_Sum'] >= 5:\n",
    "            return 'ä¸€èˆ¬ç”¨æˆ·'\n",
    "        else:\n",
    "            return 'ä½ä»·å€¼ç”¨æˆ·'\n",
    "    \n",
    "    rfm['ç”¨æˆ·åˆ†å±‚'] = rfm.apply(rfm_segment, axis=1)\n",
    "    \n",
    "    # ç»Ÿè®¡å„å±‚ç”¨æˆ·\n",
    "    segment_stats = rfm['ç”¨æˆ·åˆ†å±‚'].value_counts()\n",
    "    \n",
    "    print(\"\\nğŸ‘¥ RFMç”¨æˆ·åˆ†å±‚ç»“æœ:\")\n",
    "    print(\"-\" * 40)\n",
    "    for segment, count in segment_stats.items():\n",
    "        percentage = count / len(rfm) * 100\n",
    "        print(f\"{segment}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # æ˜¾ç¤ºå…¸å‹ç”¨æˆ·ç‰¹å¾\n",
    "    print(\"\\nğŸ¯ å„å±‚çº§å…¸å‹ç‰¹å¾:\")\n",
    "    for segment in ['é«˜ä»·å€¼ç”¨æˆ·', 'æ½œåŠ›ç”¨æˆ·', 'ä¸€èˆ¬ç”¨æˆ·', 'ä½ä»·å€¼ç”¨æˆ·']:\n",
    "        if segment in rfm['ç”¨æˆ·åˆ†å±‚'].values:\n",
    "            segment_data = rfm[rfm['ç”¨æˆ·åˆ†å±‚'] == segment]\n",
    "            print(f\"\\n{segment}:\")\n",
    "            print(f\"  å¹³å‡æœ€è¿‘è´­ä¹°å¤©æ•°: {segment_data['recency'].mean():.1f}\")\n",
    "            print(f\"  å¹³å‡è´­ä¹°æ¬¡æ•°: {segment_data['frequency'].mean():.1f}\")\n",
    "    \n",
    "    return rfm\n",
    "\n",
    "def predict_next_purchase(df):\n",
    "    \"\"\"é¢„æµ‹ç”¨æˆ·ä¸‹æ¬¡è´­ä¹°æ—¶é—´\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ”® ç”¨æˆ·ä¸‹æ¬¡è´­ä¹°æ—¶é—´é¢„æµ‹\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # åªå…³æ³¨è´­ä¹°è¡Œä¸º\n",
    "    purchase_data = df[df['behavior_type'] == 'buy'].copy()\n",
    "    \n",
    "    if len(purchase_data) < 100:\n",
    "        print(\"âŒ è´­ä¹°æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œé¢„æµ‹\")\n",
    "        return None\n",
    "    \n",
    "    # è®¡ç®—æ¯ä¸ªç”¨æˆ·çš„è´­ä¹°æ—¶é—´é—´éš”\n",
    "    user_purchases = purchase_data.sort_values(['user_id', 'timestamp'])\n",
    "    user_purchases['prev_purchase'] = user_purchases.groupby('user_id')['timestamp'].shift(1)\n",
    "    user_purchases['purchase_interval'] = (user_purchases['timestamp'] - user_purchases['prev_purchase']).dt.days\n",
    "    \n",
    "    # è¿‡æ»¤æ‰é¦–æ¬¡è´­ä¹°ï¼ˆæ— é—´éš”ï¼‰\n",
    "    interval_data = user_purchases.dropna(subset=['purchase_interval'])\n",
    "    \n",
    "    if len(interval_data) == 0:\n",
    "        print(\"âŒ æ²¡æœ‰è¶³å¤Ÿçš„è´­ä¹°é—´éš”æ•°æ®\")\n",
    "        return None\n",
    "    \n",
    "    # è®¡ç®—å¹³å‡è´­ä¹°é—´éš”\n",
    "    avg_interval = interval_data['purchase_interval'].mean()\n",
    "    median_interval = interval_data['purchase_interval'].median()\n",
    "    \n",
    "    print(f\"ğŸ“Š è´­ä¹°é—´éš”ç»Ÿè®¡:\")\n",
    "    print(f\"  å¹³å‡è´­ä¹°é—´éš”: {avg_interval:.1f} å¤©\")\n",
    "    print(f\"  ä¸­ä½æ•°è´­ä¹°é—´éš”: {median_interval:.1f} å¤©\")\n",
    "    print(f\"  æœ€çŸ­é—´éš”: {interval_data['purchase_interval'].min():.1f} å¤©\")\n",
    "    print(f\"  æœ€é•¿é—´éš”: {interval_data['purchase_interval'].max():.1f} å¤©\")\n",
    "    \n",
    "    # ç”¨æˆ·çº§ç»Ÿè®¡\n",
    "    user_intervals = interval_data.groupby('user_id')['purchase_interval'].agg(['mean', 'count']).reset_index()\n",
    "    user_intervals.columns = ['user_id', 'avg_interval', 'purchase_count']\n",
    "    \n",
    "    print(f\"\\nğŸ‘¥ ç”¨æˆ·è´­ä¹°è¡Œä¸º:\")\n",
    "    print(f\"  æœ‰å¤è´­ç”¨æˆ·æ•°: {len(user_intervals):,}\")\n",
    "    print(f\"  å¹³å‡å¤è´­æ¬¡æ•°: {user_intervals['purchase_count'].mean():.1f}\")\n",
    "    \n",
    "    # é¢„æµ‹ä¸‹æ¬¡è´­ä¹°æ—¶é—´\n",
    "    last_purchases = purchase_data.groupby('user_id')['timestamp'].max().reset_index()\n",
    "    last_purchases.columns = ['user_id', 'last_purchase']\n",
    "    \n",
    "    # åˆå¹¶å¹³å‡é—´éš”\n",
    "    user_predictions = pd.merge(last_purchases, user_intervals, on='user_id', how='left')\n",
    "    \n",
    "    # å¡«å……æ²¡æœ‰å¹³å‡é—´éš”çš„ç”¨æˆ·ä½¿ç”¨æ•´ä½“å¹³å‡\n",
    "    user_predictions['avg_interval'] = user_predictions['avg_interval'].fillna(avg_interval)\n",
    "    \n",
    "    # é¢„æµ‹ä¸‹æ¬¡è´­ä¹°æ—¶é—´\n",
    "    user_predictions['next_purchase_pred'] = user_predictions['last_purchase'] + \\\n",
    "        pd.to_timedelta(user_predictions['avg_interval'], unit='D')\n",
    "    \n",
    "    # è®¡ç®—è·ç¦»é¢„æµ‹è´­ä¹°çš„å¤©æ•°\n",
    "    analysis_date = df['timestamp'].max()\n",
    "    user_predictions['days_to_next'] = (user_predictions['next_purchase_pred'] - analysis_date).dt.days\n",
    "    \n",
    "    print(f\"\\nğŸ¯ ä¸‹æ¬¡è´­ä¹°é¢„æµ‹:\")\n",
    "    print(f\"  åˆ†æåŸºå‡†æ—¥æœŸ: {analysis_date}\")\n",
    "    print(f\"  é¢„è®¡7å¤©å†…ä¼šè´­ä¹°çš„ç”¨æˆ·: {(user_predictions['days_to_next'] <= 7).sum():,}\")\n",
    "    print(f\"  é¢„è®¡30å¤©å†…ä¼šè´­ä¹°çš„ç”¨æˆ·: {(user_predictions['days_to_next'] <= 30).sum():,}\")\n",
    "    \n",
    "    return user_predictions\n",
    "\n",
    "def product_recommendation(df):\n",
    "    \"\"\"åŸºäºç”¨æˆ·è¡Œä¸ºçš„å•†å“æ¨è\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ›ï¸  å•†å“æ¨èç³»ç»ŸåŸºç¡€\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # åˆ›å»ºç”¨æˆ·-å•†å“è¡Œä¸ºçŸ©é˜µ\n",
    "    print(\"æ„å»ºç”¨æˆ·-å•†å“è¡Œä¸ºçŸ©é˜µ...\")\n",
    "    \n",
    "    # ä¸ºä¸åŒè¡Œä¸ºç±»å‹èµ‹äºˆæƒé‡\n",
    "    behavior_weights = {'pv': 1, 'cart': 3, 'fav': 5, 'buy': 10}\n",
    "    df['behavior_weight'] = df['behavior_type'].map(behavior_weights)\n",
    "    \n",
    "    # åˆ›å»ºç”¨æˆ·-å•†å“è¯„åˆ†çŸ©é˜µ\n",
    "    user_item_matrix = df.groupby(['user_id', 'item_id'])['behavior_weight'].sum().unstack(fill_value=0)\n",
    "    \n",
    "    print(f\"çŸ©é˜µç»´åº¦: {user_item_matrix.shape[0]} ç”¨æˆ· Ã— {user_item_matrix.shape[1]} å•†å“\")\n",
    "    \n",
    "    # ç®€å•æ¨èï¼šåŸºäºç‰©å“çš„ååŒè¿‡æ»¤\n",
    "    print(\"\\nåŸºäºç‰©å“ç›¸ä¼¼åº¦çš„æ¨è:\")\n",
    "    \n",
    "    # è®¡ç®—å•†å“ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    # è½¬ç½®çŸ©é˜µï¼šå•†å“Ã—ç”¨æˆ·\n",
    "    item_user_matrix = user_item_matrix.T\n",
    "    \n",
    "    # è®¡ç®—å•†å“ç›¸ä¼¼åº¦ï¼ˆé™åˆ¶å•†å“æ•°é‡ä»¥åŠ å¿«è®¡ç®—ï¼‰\n",
    "    max_items = min(1000, item_user_matrix.shape[0])\n",
    "    item_similarity = cosine_similarity(item_user_matrix.iloc[:max_items])\n",
    "    \n",
    "    # åˆ›å»ºç›¸ä¼¼åº¦DataFrame\n",
    "    item_sim_df = pd.DataFrame(\n",
    "        item_similarity,\n",
    "        index=item_user_matrix.index[:max_items],\n",
    "        columns=item_user_matrix.index[:max_items]\n",
    "    )\n",
    "    \n",
    "    # ç¤ºä¾‹ï¼šä¸ºå‰5ä¸ªç”¨æˆ·ç”Ÿæˆæ¨è\n",
    "    print(\"\\nç¤ºä¾‹æ¨èï¼ˆå‰5ä¸ªç”¨æˆ·ï¼‰:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    sample_users = user_item_matrix.index[:5]\n",
    "    for user_id in sample_users:\n",
    "        # è·å–ç”¨æˆ·å·²äº¤äº’çš„å•†å“\n",
    "        user_items = user_item_matrix.loc[user_id]\n",
    "        interacted_items = user_items[user_items > 0].index.tolist()\n",
    "        \n",
    "        if not interacted_items:\n",
    "            continue\n",
    "        \n",
    "        # æ‰¾åˆ°ç›¸ä¼¼å•†å“\n",
    "        recommendations = {}\n",
    "        for item in interacted_items[:3]:  # å–å‰3ä¸ªäº¤äº’å•†å“\n",
    "            if item in item_sim_df.columns:\n",
    "                similar_items = item_sim_df[item].sort_values(ascending=False)[1:6]  # å–å‰5ä¸ªç›¸ä¼¼å•†å“\n",
    "                for sim_item, score in similar_items.items():\n",
    "                    if sim_item not in interacted_items:\n",
    "                        if sim_item not in recommendations:\n",
    "                            recommendations[sim_item] = 0\n",
    "                        recommendations[sim_item] += score\n",
    "        \n",
    "        # æ’åºæ¨è\n",
    "        if recommendations:\n",
    "            top_recommendations = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "            print(f\"\\nç”¨æˆ· {user_id} çš„æ¨èå•†å“:\")\n",
    "            for item_id, score in top_recommendations:\n",
    "                print(f\"  å•†å“{item_id} (ç›¸ä¼¼åº¦: {score:.3f})\")\n",
    "    \n",
    "    return user_item_matrix, item_sim_df\n",
    "\n",
    "def inventory_optimization(df):\n",
    "    \"\"\"åº“å­˜ä¼˜åŒ–å»ºè®®\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“¦ åº“å­˜ä¼˜åŒ–åˆ†æ\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # åˆ†æå•†å“é”€å”®æ¨¡å¼\n",
    "    product_sales = df[df['behavior_type'] == 'buy'].groupby('item_id').agg({\n",
    "        'user_id': 'count',  # é”€å”®æ•°é‡\n",
    "        'date': ['min', 'max', 'nunique']  # é”€å”®æ—¶é—´èŒƒå›´\n",
    "    }).reset_index()\n",
    "    \n",
    "    # æ‰å¹³åŒ–åˆ—å\n",
    "    product_sales.columns = ['item_id', 'sales_count', 'first_sale', 'last_sale', 'sale_days']\n",
    "    \n",
    "    # è®¡ç®—æ—¥å‡é”€é‡\n",
    "    product_sales['days_range'] = (product_sales['last_sale'] - product_sales['first_sale']).dt.days + 1\n",
    "    product_sales['daily_sales'] = product_sales['sales_count'] / product_sales['days_range'].clip(lower=1)\n",
    "    \n",
    "    # å•†å“åˆ†ç±»ï¼šåŸºäºé”€é‡å’Œé”€å”®é¢‘ç‡\n",
    "    def classify_product(row):\n",
    "        if row['sales_count'] >= 100 and row['daily_sales'] >= 5:\n",
    "            return 'çƒ­é”€å“'\n",
    "        elif row['sales_count'] >= 50:\n",
    "            return 'å¸¸é”€å“'\n",
    "        elif row['sales_count'] >= 10:\n",
    "            return 'ä¸€èˆ¬å“'\n",
    "        else:\n",
    "            return 'æ»é”€å“'\n",
    "    \n",
    "    product_sales['product_type'] = product_sales.apply(classify_product, axis=1)\n",
    "    \n",
    "    print(\"ğŸ“Š å•†å“åˆ†ç±»ç»Ÿè®¡:\")\n",
    "    print(\"-\" * 40)\n",
    "    type_stats = product_sales['product_type'].value_counts()\n",
    "    for ptype, count in type_stats.items():\n",
    "        percentage = count / len(product_sales) * 100\n",
    "        print(f\"{ptype}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # åº“å­˜å»ºè®®\n",
    "    print(\"\\nğŸ’¡ åº“å­˜ä¼˜åŒ–å»ºè®®:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # çƒ­é”€å“å»ºè®®\n",
    "    hot_products = product_sales[product_sales['product_type'] == 'çƒ­é”€å“']\n",
    "    if len(hot_products) > 0:\n",
    "        avg_daily_sales = hot_products['daily_sales'].mean()\n",
    "        print(f\"1. çƒ­é”€å“({len(hot_products)}ä¸ª):\")\n",
    "        print(f\"   - å¹³å‡æ—¥é”€é‡: {avg_daily_sales:.1f}\")\n",
    "        print(f\"   - å»ºè®®å®‰å…¨åº“å­˜: {avg_daily_sales * 7:.0f} (7å¤©é”€é‡)\")\n",
    "        print(f\"   - å»ºè®®è¡¥è´§å‘¨æœŸ: æ¯å‘¨2-3æ¬¡\")\n",
    "    \n",
    "    # æ»é”€å“å»ºè®®\n",
    "    slow_products = product_sales[product_sales['product_type'] == 'æ»é”€å“']\n",
    "    if len(slow_products) > 0:\n",
    "        print(f\"\\n2. æ»é”€å“({len(slow_products)}ä¸ª):\")\n",
    "        print(f\"   - æ€»æ»é”€å•†å“æ•°: {len(slow_products):,}\")\n",
    "        print(f\"   - å»ºè®®: æ¸…ä»“ä¿ƒé”€æˆ–ä¸‹æ¶\")\n",
    "        print(f\"   - å¯é‡Šæ”¾åº“å­˜ç©ºé—´æ½œåŠ›å¤§\")\n",
    "    \n",
    "    # ABCåˆ†æï¼ˆå¸•ç´¯æ‰˜åˆ†æï¼‰\n",
    "    product_sales = product_sales.sort_values('sales_count', ascending=False)\n",
    "    product_sales['cumulative_percentage'] = product_sales['sales_count'].cumsum() / product_sales['sales_count'].sum() * 100\n",
    "    \n",
    "    # Aç±»å•†å“ï¼ˆå‰20%è´¡çŒ®80%é”€é‡ï¼‰\n",
    "    a_products = product_sales[product_sales['cumulative_percentage'] <= 80]\n",
    "    b_products = product_sales[(product_sales['cumulative_percentage'] > 80) & (product_sales['cumulative_percentage'] <= 95)]\n",
    "    c_products = product_sales[product_sales['cumulative_percentage'] > 95]\n",
    "    \n",
    "    print(f\"\\n3. ABCåˆ†æ:\")\n",
    "    print(f\"   Aç±»å•†å“({len(a_products)}ä¸ª): è´¡çŒ®80%é”€é‡ï¼Œé‡ç‚¹ç®¡ç†\")\n",
    "    print(f\"   Bç±»å•†å“({len(b_products)}ä¸ª): è´¡çŒ®15%é”€é‡ï¼Œå¸¸è§„ç®¡ç†\")\n",
    "    print(f\"   Cç±»å•†å“({len(c_products)}ä¸ª): è´¡çŒ®5%é”€é‡ï¼Œç®€åŒ–ç®¡ç†\")\n",
    "    \n",
    "    return product_sales\n",
    "\n",
    "def generate_advanced_report(df, retention_matrix, rfm_result, \n",
    "                           purchase_predictions, product_sales):\n",
    "    \"\"\"ç”Ÿæˆé«˜çº§åˆ†ææŠ¥å‘Š\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“‹ ç”Ÿæˆé«˜çº§åˆ†ææŠ¥å‘Š\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    report_path = r'D:\\Mycode\\advanced_analysis_report.md'\n",
    "    \n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"# ç”µå•†é«˜çº§åˆ†ææŠ¥å‘Š\\n\\n\")\n",
    "        f.write(f\"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"åˆ†ææ•°æ®: {len(df):,} è¡Œ\\n\\n\")\n",
    "        \n",
    "        f.write(\"## ğŸ“ˆ ç”¨æˆ·ç•™å­˜åˆ†æ\\n\\n\")\n",
    "        if retention_matrix is not None:\n",
    "            if 1 in retention_matrix.columns:\n",
    "                f.write(f\"- **æ¬¡æ—¥ç•™å­˜ç‡**: {retention_matrix[1].mean()*100:.1f}%\\n\")\n",
    "            if 7 in retention_matrix.columns:\n",
    "                f.write(f\"- **7æ—¥ç•™å­˜ç‡**: {retention_matrix[7].mean()*100:.1f}%\\n\")\n",
    "            if 30 in retention_matrix.columns:\n",
    "                f.write(f\"- **30æ—¥ç•™å­˜ç‡**: {retention_matrix[30].mean()*100:.1f}%\\n\")\n",
    "        \n",
    "        f.write(\"\\n## ğŸ’° RFMç”¨æˆ·ä»·å€¼åˆ†æ\\n\\n\")\n",
    "        if rfm_result is not None:\n",
    "            segment_stats = rfm_result['ç”¨æˆ·åˆ†å±‚'].value_counts()\n",
    "            for segment, count in segment_stats.items():\n",
    "                percentage = count / len(rfm_result) * 100\n",
    "                f.write(f\"- **{segment}**: {count:,} ({percentage:.1f}%)\\n\")\n",
    "            \n",
    "            f.write(f\"\\n**é«˜ä»·å€¼ç”¨æˆ·ç‰¹å¾**:\\n\")\n",
    "            high_value = rfm_result[rfm_result['ç”¨æˆ·åˆ†å±‚'] == 'é«˜ä»·å€¼ç”¨æˆ·']\n",
    "            if len(high_value) > 0:\n",
    "                f.write(f\"  - å¹³å‡æœ€è¿‘è´­ä¹°: {high_value['recency'].mean():.1f} å¤©å‰\\n\")\n",
    "                f.write(f\"  - å¹³å‡è´­ä¹°æ¬¡æ•°: {high_value['frequency'].mean():.1f} æ¬¡\\n\")\n",
    "        \n",
    "        f.write(\"\\n## ğŸ”® è´­ä¹°é¢„æµ‹åˆ†æ\\n\\n\")\n",
    "        if purchase_predictions is not None:\n",
    "            analysis_date = df['timestamp'].max()\n",
    "            upcoming_7 = (purchase_predictions['days_to_next'] <= 7).sum()\n",
    "            upcoming_30 = (purchase_predictions['days_to_next'] <= 30).sum()\n",
    "            \n",
    "            f.write(f\"- **é¢„æµ‹åŸºå‡†æ—¥æœŸ**: {analysis_date.strftime('%Y-%m-%d')}\\n\")\n",
    "            f.write(f\"- **7å¤©å†…å¯èƒ½è´­ä¹°ç”¨æˆ·**: {upcoming_7:,}\\n\")\n",
    "            f.write(f\"- **30å¤©å†…å¯èƒ½è´­ä¹°ç”¨æˆ·**: {upcoming_30:,}\\n\")\n",
    "        \n",
    "        f.write(\"\\n## ğŸ“¦ åº“å­˜ä¼˜åŒ–å»ºè®®\\n\\n\")\n",
    "        if product_sales is not None:\n",
    "            type_stats = product_sales['product_type'].value_counts()\n",
    "            for ptype, count in type_stats.items():\n",
    "                percentage = count / len(product_sales) * 100\n",
    "                f.write(f\"- **{ptype}**: {count:,} ({percentage:.1f}%)\\n\")\n",
    "            \n",
    "            # ABCåˆ†æ\n",
    "            product_sales_sorted = product_sales.sort_values('sales_count', ascending=False)\n",
    "            cumulative_pct = product_sales_sorted['sales_count'].cumsum() / product_sales_sorted['sales_count'].sum() * 100\n",
    "            \n",
    "            a_count = (cumulative_pct <= 80).sum()\n",
    "            b_count = ((cumulative_pct > 80) & (cumulative_pct <= 95)).sum()\n",
    "            c_count = (cumulative_pct > 95).sum()\n",
    "            \n",
    "            f.write(f\"\\n**ABCåº“å­˜åˆ†ç±»**:\\n\")\n",
    "            f.write(f\"  - Aç±»å•†å“(é‡ç‚¹): {a_count:,} ä¸ª\\n\")\n",
    "            f.write(f\"  - Bç±»å•†å“(å¸¸è§„): {b_count:,} ä¸ª\\n\")\n",
    "            f.write(f\"  - Cç±»å•†å“(ç®€åŒ–): {c_count:,} ä¸ª\\n\")\n",
    "        \n",
    "        f.write(\"\\n## ğŸ¯ ä¸šåŠ¡è¡ŒåŠ¨å»ºè®®\\n\\n\")\n",
    "        \n",
    "        f.write(\"### 1. æå‡ç”¨æˆ·ç•™å­˜\\n\")\n",
    "        f.write(\"- **ç›®æ ‡**: å°†7æ—¥ç•™å­˜ç‡æå‡è‡³XX%\\n\")\n",
    "        f.write(\"- **æªæ–½**:\\n\")\n",
    "        f.write(\"  - æ–°ç”¨æˆ·å¼•å¯¼æµç¨‹ä¼˜åŒ–\\n\")\n",
    "        f.write(\"  - æ¬¡æ—¥å›è®¿æé†’æœºåˆ¶\\n\")\n",
    "        f.write(\"  - 7æ—¥ç­¾åˆ°å¥–åŠ±æ´»åŠ¨\\n\\n\")\n",
    "        \n",
    "        f.write(\"### 2. é«˜ä»·å€¼ç”¨æˆ·è¿è¥\\n\")\n",
    "        f.write(\"- **ç›®æ ‡**: é«˜ä»·å€¼ç”¨æˆ·å æ¯”æå‡è‡³XX%\\n\")\n",
    "        f.write(\"- **æªæ–½**:\\n\")\n",
    "        f.write(\"  - VIPä¸“å±æƒç›Šä½“ç³»\\n\")\n",
    "        f.write(\"  - ä¸ªæ€§åŒ–æ¨èå’Œä¼˜æƒ \\n\")\n",
    "        f.write(\"  - å®šæœŸå…³æ€€å’Œå›è®¿\\n\\n\")\n",
    "        \n",
    "        f.write(\"### 3. ç²¾å‡†è¥é”€é¢„æµ‹\\n\")\n",
    "        f.write(\"- **ç›®æ ‡**: æé«˜é¢„æµ‹å‡†ç¡®ç‡ï¼Œé™ä½è¥é”€æˆæœ¬\\n\")\n",
    "        f.write(\"- **æªæ–½**:\\n\")\n",
    "        f.write(\"  - åŸºäºé¢„æµ‹ç»“æœçš„ç²¾å‡†æ¨é€\\n\")\n",
    "        f.write(\"  - è´­ä¹°å‘¨æœŸæé†’æœåŠ¡\\n\")\n",
    "        f.write(\"  - æµå¤±é¢„è­¦å’ŒæŒ½å›æœºåˆ¶\\n\\n\")\n",
    "        \n",
    "        f.write(\"### 4. æ™ºèƒ½åº“å­˜ç®¡ç†\\n\")\n",
    "        f.write(\"- **ç›®æ ‡**: åº“å­˜å‘¨è½¬ç‡æå‡XX%\\n\")\n",
    "        f.write(\"- **æªæ–½**:\\n\")\n",
    "        f.write(\"  - Aç±»å•†å“é‡ç‚¹å¤‡è´§\\n\")\n",
    "        f.write(\"  - Cç±»å•†å“å‡å°‘åº“å­˜\\n\")\n",
    "        f.write(\"  - åŠ¨æ€å®‰å…¨åº“å­˜è°ƒæ•´\\n\\n\")\n",
    "        \n",
    "        f.write(\"## ğŸ“Š åˆ†æè¯´æ˜\\n\")\n",
    "        f.write(\"- æœ¬åˆ†æåŸºäºç”¨æˆ·è¡Œä¸ºæ•°æ®è¿›è¡Œæ·±åº¦æŒ–æ˜\\n\")\n",
    "        f.write(\"- æ‰€æœ‰å»ºè®®éœ€ç»“åˆå…·ä½“ä¸šåŠ¡æƒ…å†µè°ƒæ•´\\n\")\n",
    "        f.write(\"- å»ºè®®å»ºç«‹æ•°æ®ç›‘æ§ä½“ç³»ï¼ŒæŒç»­ä¼˜åŒ–\\n\")\n",
    "    \n",
    "    print(f\"âœ… é«˜çº§åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_path}\")\n",
    "    return report_path\n",
    "\n",
    "def main():\n",
    "    \"\"\"é«˜çº§åˆ†æä¸»æµç¨‹\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"ğŸš€ ç”µå•†é«˜çº§åˆ†æ - ç”¨æˆ·ç•™å­˜ä¸ä»·å€¼æŒ–æ˜\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    import os\n",
    "    \n",
    "    # 1. åŠ è½½æ•°æ®\n",
    "    df = load_and_prepare_data()\n",
    "    if df is None:\n",
    "        return\n",
    "    \n",
    "    # 2. ç”¨æˆ·ç•™å­˜åˆ†æ\n",
    "    retention_matrix, user_activity = calculate_user_retention(df)\n",
    "    \n",
    "    # 3. RFMç”¨æˆ·ä»·å€¼åˆ†æ\n",
    "    rfm_result = rfm_analysis(df)\n",
    "    \n",
    "    # 4. è´­ä¹°æ—¶é—´é¢„æµ‹\n",
    "    purchase_predictions = predict_next_purchase(df)\n",
    "    \n",
    "    # 5. å•†å“æ¨èåŸºç¡€ï¼ˆå¯é€‰ï¼Œè®¡ç®—è¾ƒæ…¢ï¼‰\n",
    "    # user_item_matrix, item_sim_df = product_recommendation(df)\n",
    "    \n",
    "    # 6. åº“å­˜ä¼˜åŒ–åˆ†æ\n",
    "    product_sales = inventory_optimization(df)\n",
    "    \n",
    "    # 7. ç”Ÿæˆé«˜çº§æŠ¥å‘Š\n",
    "    report_path = generate_advanced_report(df, retention_matrix, rfm_result,\n",
    "                                         purchase_predictions, product_sales)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ‰ é«˜çº§åˆ†æå®Œæˆï¼\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"ğŸ“ è¾“å‡ºæˆæœ:\")\n",
    "    print(f\"1. é«˜çº§åˆ†ææŠ¥å‘Š: {report_path}\")\n",
    "    print(\"\\nğŸ” æ ¸å¿ƒæ´å¯Ÿ:\")\n",
    "    if retention_matrix is not None and 7 in retention_matrix.columns:\n",
    "        print(f\"- 7æ—¥ç”¨æˆ·ç•™å­˜ç‡: {retention_matrix[7].mean()*100:.1f}%\")\n",
    "    if rfm_result is not None:\n",
    "        high_value = rfm_result[rfm_result['ç”¨æˆ·åˆ†å±‚'] == 'é«˜ä»·å€¼ç”¨æˆ·']\n",
    "        print(f\"- é«˜ä»·å€¼ç”¨æˆ·å æ¯”: {len(high_value)/len(rfm_result)*100:.1f}%\")\n",
    "    if product_sales is not None:\n",
    "        hot_products = product_sales[product_sales['product_type'] == 'çƒ­é”€å“']\n",
    "        print(f\"- çƒ­é”€å•†å“æ•°é‡: {len(hot_products):,}\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b718c5e-e808-4312-8a41-ca568524439b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ¢ ä¼ä¸šçº§ç”µå•†æ•°æ®åˆ†æç³»ç»Ÿå¯åŠ¨\n",
      "======================================================================\n",
      "======================================================================\n",
      "ğŸ¢ ä¼ä¸šçº§ç”µå•†æ•°æ®åˆ†æç³»ç»Ÿ\n",
      "======================================================================\n",
      "åŸºç¡€è·¯å¾„: D:\\Mycode\n",
      "åŸå§‹æ•°æ®: D:\\Mycode\\UserBehavior.csv\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ å¯åŠ¨ä¼ä¸šçº§ç”µå•†åˆ†æ\n",
      "======================================================================\n",
      "\n",
      "ğŸ” æ•°æ®å®Œæ•´æ€§æ£€æŸ¥...\n",
      "âœ… æ•°æ®æ–‡ä»¶å­˜åœ¨ï¼Œå¤§å°: 3.42 GB\n",
      "\n",
      "ğŸ”„ æ•°æ®å¤„ç†æµæ°´çº¿...\n",
      "æ­¥éª¤1: æ•°æ®æ¸…æ´—å’Œè½¬æ¢...\n",
      "  å·²å¤„ç†: 500,000 è¡Œ\n",
      "  å·²å¤„ç†: 1,000,000 è¡Œ\n",
      "  å·²å¤„ç†: 1,500,000 è¡Œ\n",
      "  å·²å¤„ç†: 2,000,000 è¡Œ\n",
      "  è¾¾åˆ°æ ·æœ¬é™åˆ¶: 2,000,000 è¡Œ\n",
      "âœ… æ•°æ®å¤„ç†å®Œæˆ: 2,000,000 è¡Œ\n",
      "\n",
      "ğŸ“Š æ‰§è¡Œä¼ä¸šçº§åˆ†æ...\n",
      "åŠ è½½åˆ†ææ•°æ®...\n",
      "  ç”¨æˆ·è¡Œä¸ºåˆ†æ...\n",
      "  é”€å”®è¡¨ç°åˆ†æ...\n",
      "  å•†å“ç»„åˆåˆ†æ...\n",
      "  å•†å“ç»„åˆåˆ†æå¤±è´¥: Bin edges must be unique: Index([0.0, 1.0, 1.0, 2.0, 285.0], dtype='float64', name='views').\n",
      "You can drop duplicate edges by setting the 'duplicates' kwarg\n",
      "  è½¬åŒ–æ¼æ–—åˆ†æ...\n",
      "  æ—¶é—´æ¨¡å¼åˆ†æ...\n",
      "  ç”¨æˆ·ä»·å€¼åˆ†æ...\n",
      "  åº“å­˜ä¼˜åŒ–åˆ†æ...\n",
      "\n",
      "ğŸ“‹ ç”Ÿæˆä¼ä¸šæŠ¥å‘Š...\n",
      "âœ… ä¼ä¸šæŠ¥å‘Šç”Ÿæˆå®Œæˆ: D:\\Mycode\\enterprise_reports\\enterprise_report_20260131_224101.md\n",
      "\n",
      "ğŸ¨ åˆ›å»ºä¸“ä¸šä»ªè¡¨æ¿...\n",
      "  åˆ›å»ºä¸“ä¸šå¯è§†åŒ–å›¾è¡¨...\n",
      "  âœ… ä¸“ä¸šä»ªè¡¨æ¿å›¾è¡¨: D:\\Mycode\\enterprise_results\\enterprise_dashboard\\enterprise_dashboard.png\n",
      "  âœ… ä¼ä¸šçº§HTMLä»ªè¡¨æ¿: D:\\Mycode\\enterprise_results\\enterprise_dashboard\\enterprise_dashboard.html\n",
      "\n",
      "======================================================================\n",
      "ğŸ‰ ä¼ä¸šçº§åˆ†æå®Œæˆï¼\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ ä¼ä¸šçº§æˆæœ:\n",
      "1. åˆ†ææŠ¥å‘Š: D:\\Mycode\\enterprise_reports\\enterprise_report_20260131_224101.md\n",
      "2. ä»ªè¡¨æ¿: D:\\Mycode\\enterprise_results\\enterprise_dashboard\\enterprise_dashboard.html\n",
      "3. å¤„ç†æ•°æ®: D:\\Mycode\\enterprise_processed\n",
      "4. åˆ†æç»“æœ: D:\\Mycode\\enterprise_results\n",
      "\n",
      "======================================================================\n",
      "âœ… ä¼ä¸šçº§åˆ†ææˆåŠŸå®Œæˆï¼\n",
      "======================================================================\n",
      "\n",
      "ğŸ¯ ä½ å¯ä»¥ï¼š\n",
      "1. æŸ¥çœ‹ä¼ä¸šæŠ¥å‘Š: D:\\Mycode\\enterprise_reports\\enterprise_report_20260131_224101.md\n",
      "2. æ‰“å¼€HTMLä»ªè¡¨æ¿æŸ¥çœ‹å¯è§†åŒ–\n",
      "3. åŸºäºæˆ˜ç•¥å»ºè®®åˆ¶å®šä¸šåŠ¡è®¡åˆ’\n",
      "4. ä¸å›¢é˜Ÿåˆ†äº«åˆ†ææˆæœ\n",
      "\n",
      "======================================================================\n",
      "ğŸ ç³»ç»Ÿæ‰§è¡Œå®Œæˆ\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ç”µå•†æ•°æ®åˆ†æç³»ç»Ÿ - ä¼ä¸šçº§å®Œæ•´ç‰ˆ\n",
    "ä¿®å¤æ‰€æœ‰è¯­æ³•é”™è¯¯ï¼Œå¯ç›´æ¥è¿è¡Œ\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡å­—ä½“\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "class EnterpriseEcommerceSystem:\n",
    "    \"\"\"ä¼ä¸šçº§ç”µå•†æ•°æ®åˆ†æç³»ç»Ÿ\"\"\"\n",
    "    \n",
    "    def __init__(self, base_path: str = r'D:\\Mycode'):\n",
    "        self.base_path = base_path\n",
    "        self.raw_data_path = os.path.join(base_path, 'UserBehavior.csv')\n",
    "        self.processed_dir = os.path.join(base_path, 'enterprise_processed')\n",
    "        self.results_dir = os.path.join(base_path, 'enterprise_results')\n",
    "        self.reports_dir = os.path.join(base_path, 'enterprise_reports')\n",
    "        \n",
    "        # åˆ›å»ºç›®å½•ç»“æ„\n",
    "        for directory in [self.processed_dir, self.results_dir, self.reports_dir]:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(\"ğŸ¢ ä¼ä¸šçº§ç”µå•†æ•°æ®åˆ†æç³»ç»Ÿ\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"åŸºç¡€è·¯å¾„: {base_path}\")\n",
    "        print(f\"åŸå§‹æ•°æ®: {self.raw_data_path}\")\n",
    "    \n",
    "    def check_data_integrity(self) -> bool:\n",
    "        \"\"\"æ£€æŸ¥æ•°æ®å®Œæ•´æ€§\"\"\"\n",
    "        print(\"\\nğŸ” æ•°æ®å®Œæ•´æ€§æ£€æŸ¥...\")\n",
    "        \n",
    "        if not os.path.exists(self.raw_data_path):\n",
    "            print(f\"âŒ åŸå§‹æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.raw_data_path}\")\n",
    "            return False\n",
    "        \n",
    "        file_size = os.path.getsize(self.raw_data_path) / (1024**3)\n",
    "        print(f\"âœ… æ•°æ®æ–‡ä»¶å­˜åœ¨ï¼Œå¤§å°: {file_size:.2f} GB\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def run_enterprise_analysis(self):\n",
    "        \"\"\"è¿è¡Œä¼ä¸šçº§åˆ†æ\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ğŸš€ å¯åŠ¨ä¼ä¸šçº§ç”µå•†åˆ†æ\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # 1. æ•°æ®æ£€æŸ¥\n",
    "        if not self.check_data_integrity():\n",
    "            print(\"æ•°æ®æ£€æŸ¥å¤±è´¥ï¼Œç»ˆæ­¢åˆ†æ\")\n",
    "            return\n",
    "        \n",
    "        # 2. æ•°æ®å¤„ç†æµæ°´çº¿\n",
    "        print(\"\\nğŸ”„ æ•°æ®å¤„ç†æµæ°´çº¿...\")\n",
    "        processed_data = self.data_processing_pipeline(sample_size=2000000)\n",
    "        \n",
    "        if processed_data is None:\n",
    "            print(\"æ•°æ®å¤„ç†å¤±è´¥\")\n",
    "            return\n",
    "        \n",
    "        # 3. ä¼ä¸šçº§åˆ†æ\n",
    "        print(\"\\nğŸ“Š æ‰§è¡Œä¼ä¸šçº§åˆ†æ...\")\n",
    "        analysis_results = self.perform_enterprise_analysis(processed_data)\n",
    "        \n",
    "        # 4. ç”Ÿæˆä¼ä¸šæŠ¥å‘Š\n",
    "        print(\"\\nğŸ“‹ ç”Ÿæˆä¼ä¸šæŠ¥å‘Š...\")\n",
    "        report_path = self.generate_enterprise_report(processed_data, analysis_results)\n",
    "        \n",
    "        # 5. åˆ›å»ºä¸“ä¸šä»ªè¡¨æ¿\n",
    "        print(\"\\nğŸ¨ åˆ›å»ºä¸“ä¸šä»ªè¡¨æ¿...\")\n",
    "        dashboard_path = self.create_professional_dashboard(processed_data, analysis_results)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ğŸ‰ ä¼ä¸šçº§åˆ†æå®Œæˆï¼\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\nğŸ“ ä¼ä¸šçº§æˆæœ:\")\n",
    "        print(f\"1. åˆ†ææŠ¥å‘Š: {report_path}\")\n",
    "        print(f\"2. ä»ªè¡¨æ¿: {dashboard_path}\")\n",
    "        print(f\"3. å¤„ç†æ•°æ®: {self.processed_dir}\")\n",
    "        print(f\"4. åˆ†æç»“æœ: {self.results_dir}\")\n",
    "        \n",
    "        return report_path\n",
    "    \n",
    "    def data_processing_pipeline(self, sample_size: int = 5000000):\n",
    "        \"\"\"ä¼ä¸šçº§æ•°æ®å¤„ç†æµæ°´çº¿\"\"\"\n",
    "        print(\"æ­¥éª¤1: æ•°æ®æ¸…æ´—å’Œè½¬æ¢...\")\n",
    "        \n",
    "        output_path = os.path.join(self.processed_dir, 'enterprise_cleaned.csv')\n",
    "        column_names = ['user_id', 'item_id', 'category_id', 'behavior_type', 'timestamp']\n",
    "        chunk_size = 500000\n",
    "        \n",
    "        first_batch = True\n",
    "        total_rows = 0\n",
    "        \n",
    "        try:\n",
    "            for chunk in pd.read_csv(self.raw_data_path, names=column_names, chunksize=chunk_size):\n",
    "                # ä¼ä¸šçº§æ•°æ®æ¸…æ´—\n",
    "                chunk = self.enterprise_data_cleaning(chunk)\n",
    "                \n",
    "                if len(chunk) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # æ—¶é—´è½¬æ¢\n",
    "                chunk['timestamp'] = pd.to_datetime(chunk['timestamp'], unit='s', errors='coerce')\n",
    "                chunk = chunk.dropna(subset=['timestamp'])\n",
    "                \n",
    "                # ä¼ä¸šçº§ç‰¹å¾æå–\n",
    "                chunk = self.extract_enterprise_features(chunk)\n",
    "                \n",
    "                # ä¿å­˜\n",
    "                mode = 'w' if first_batch else 'a'\n",
    "                header = first_batch\n",
    "                \n",
    "                chunk.to_csv(output_path, index=False, mode=mode, header=header)\n",
    "                \n",
    "                first_batch = False\n",
    "                total_rows += len(chunk)\n",
    "                \n",
    "                print(f\"  å·²å¤„ç†: {total_rows:,} è¡Œ\")\n",
    "                \n",
    "                if total_rows >= sample_size:\n",
    "                    print(f\"  è¾¾åˆ°æ ·æœ¬é™åˆ¶: {sample_size:,} è¡Œ\")\n",
    "                    break\n",
    "            \n",
    "            print(f\"âœ… æ•°æ®å¤„ç†å®Œæˆ: {total_rows:,} è¡Œ\")\n",
    "            return output_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æ•°æ®å¤„ç†å¤±è´¥: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def enterprise_data_cleaning(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"ä¼ä¸šçº§æ•°æ®æ¸…æ´—\"\"\"\n",
    "        # 1. ç§»é™¤é‡å¤\n",
    "        df = df.drop_duplicates()\n",
    "        \n",
    "        # 2. å¤„ç†ç¼ºå¤±å€¼\n",
    "        df = df.dropna()\n",
    "        \n",
    "        # 3. æ•°æ®ç±»å‹è½¬æ¢\n",
    "        df['user_id'] = pd.to_numeric(df['user_id'], errors='coerce')\n",
    "        df['item_id'] = pd.to_numeric(df['item_id'], errors='coerce')\n",
    "        df['category_id'] = pd.to_numeric(df['category_id'], errors='coerce')\n",
    "        \n",
    "        # 4. æ•°æ®éªŒè¯\n",
    "        df = df[\n",
    "            (df['user_id'] > 0) & \n",
    "            (df['item_id'] > 0) & \n",
    "            (df['category_id'] > 0)\n",
    "        ]\n",
    "        \n",
    "        # 5. è¡Œä¸ºç±»å‹éªŒè¯\n",
    "        valid_behaviors = ['pv', 'cart', 'fav', 'buy']\n",
    "        df = df[df['behavior_type'].isin(valid_behaviors)]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def extract_enterprise_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"æå–ä¼ä¸šçº§ç‰¹å¾\"\"\"\n",
    "        # åŸºç¡€æ—¶é—´ç‰¹å¾\n",
    "        df['date'] = df['timestamp'].dt.date\n",
    "        df['year'] = df['timestamp'].dt.year\n",
    "        df['month'] = df['timestamp'].dt.month\n",
    "        df['day'] = df['timestamp'].dt.day\n",
    "        df['hour'] = df['timestamp'].dt.hour\n",
    "        df['weekday'] = df['timestamp'].dt.weekday\n",
    "        df['week'] = df['timestamp'].dt.isocalendar().week\n",
    "        \n",
    "        # ä¸šåŠ¡ç‰¹å¾\n",
    "        df['is_weekend'] = df['weekday'].isin([5, 6]).astype(int)\n",
    "        df['is_peak_hour'] = df['hour'].between(10, 22).astype(int)\n",
    "        df['is_working_hour'] = df['hour'].between(9, 18).astype(int)\n",
    "        \n",
    "        # ä¼šè¯ç‰¹å¾ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰\n",
    "        df['date_hour'] = df['date'].astype(str) + '_' + df['hour'].astype(str)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def perform_enterprise_analysis(self, data_path: str) -> Dict:\n",
    "        \"\"\"æ‰§è¡Œä¼ä¸šçº§åˆ†æ\"\"\"\n",
    "        print(\"åŠ è½½åˆ†ææ•°æ®...\")\n",
    "        df = pd.read_csv(data_path, nrows=1000000)\n",
    "        \n",
    "        # è½¬æ¢æ—¥æœŸåˆ—\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # ä¼ä¸šçº§åˆ†ææ¨¡å—\n",
    "        analysis_modules = [\n",
    "            ('ç”¨æˆ·è¡Œä¸ºåˆ†æ', self.analyze_user_behavior_enterprise),\n",
    "            ('é”€å”®è¡¨ç°åˆ†æ', self.analyze_sales_performance_enterprise),\n",
    "            ('å•†å“ç»„åˆåˆ†æ', self.analyze_product_portfolio_enterprise),\n",
    "            ('è½¬åŒ–æ¼æ–—åˆ†æ', self.analyze_conversion_funnel_enterprise),\n",
    "            ('æ—¶é—´æ¨¡å¼åˆ†æ', self.analyze_temporal_patterns_enterprise),\n",
    "            ('ç”¨æˆ·ä»·å€¼åˆ†æ', self.analyze_user_value_rfm),\n",
    "            ('åº“å­˜ä¼˜åŒ–åˆ†æ', self.analyze_inventory_optimization)\n",
    "        ]\n",
    "        \n",
    "        for name, analysis_func in analysis_modules:\n",
    "            try:\n",
    "                print(f\"  {name}...\")\n",
    "                result = analysis_func(df)\n",
    "                results[name] = result\n",
    "            except Exception as e:\n",
    "                print(f\"  {name}å¤±è´¥: {e}\")\n",
    "                results[name] = {'error': str(e)}\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def analyze_user_behavior_enterprise(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"ä¼ä¸šçº§ç”¨æˆ·è¡Œä¸ºåˆ†æ\"\"\"\n",
    "        result = {\n",
    "            'åŸºç¡€ç»Ÿè®¡': {\n",
    "                'æ€»ç”¨æˆ·æ•°': int(df['user_id'].nunique()),\n",
    "                'æ€»è¡Œä¸ºæ•°': int(len(df)),\n",
    "                'æ—¥å‡æ´»è·ƒç”¨æˆ·': float(df.groupby('date')['user_id'].nunique().mean()),\n",
    "                'ç”¨æˆ·å¹³å‡è¡Œä¸ºæ•°': float(df.groupby('user_id').size().mean())\n",
    "            },\n",
    "            'è¡Œä¸ºåˆ†å¸ƒ': df['behavior_type'].value_counts().to_dict(),\n",
    "            'ç”¨æˆ·æ´»è·ƒåº¦': {\n",
    "                'é«˜é¢‘ç”¨æˆ·(>50æ¬¡)': int((df.groupby('user_id').size() > 50).sum()),\n",
    "                'ä¸­é¢‘ç”¨æˆ·(10-50æ¬¡)': int(((df.groupby('user_id').size() >= 10) & \n",
    "                                      (df.groupby('user_id').size() <= 50)).sum()),\n",
    "                'ä½é¢‘ç”¨æˆ·(<10æ¬¡)': int((df.groupby('user_id').size() < 10).sum())\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def analyze_sales_performance_enterprise(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"ä¼ä¸šçº§é”€å”®è¡¨ç°åˆ†æ\"\"\"\n",
    "        purchases = df[df['behavior_type'] == 'buy']\n",
    "        \n",
    "        if len(purchases) == 0:\n",
    "            return {'error': 'æ— è´­ä¹°æ•°æ®'}\n",
    "        \n",
    "        daily_sales = purchases.groupby('date').size()\n",
    "        hourly_sales = purchases.groupby('hour').size()\n",
    "        \n",
    "        result = {\n",
    "            'é”€å”®æ¦‚è§ˆ': {\n",
    "                'æ€»è´­ä¹°æ¬¡æ•°': int(len(purchases)),\n",
    "                'è´­ä¹°ç”¨æˆ·æ•°': int(purchases['user_id'].nunique()),\n",
    "                'æ—¥å‡è´­ä¹°é‡': float(daily_sales.mean()),\n",
    "                'è´­ä¹°å•†å“ç§ç±»': int(purchases['item_id'].nunique())\n",
    "            },\n",
    "            'æ—¶é—´è¶‹åŠ¿': {\n",
    "                'é”€å”®é«˜å³°æ—¶æ®µ': int(hourly_sales.idxmax()),\n",
    "                'é”€å”®é«˜å³°å€¼': int(hourly_sales.max()),\n",
    "                'é”€å”®ä½è°·æ—¶æ®µ': int(hourly_sales.idxmin()),\n",
    "                'å‘¨æœ«é”€å”®å æ¯”': float(purchases[purchases['is_weekend'] == 1].shape[0] / len(purchases) * 100)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def analyze_product_portfolio_enterprise(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"ä¼ä¸šçº§å•†å“ç»„åˆåˆ†æ\"\"\"\n",
    "        # å•†å“è¡¨ç°\n",
    "        product_stats = df.groupby('item_id').agg({\n",
    "            'behavior_type': lambda x: {\n",
    "                'views': (x == 'pv').sum(),\n",
    "                'carts': (x == 'cart').sum(),\n",
    "                'favorites': (x == 'fav').sum(),\n",
    "                'purchases': (x == 'buy').sum()\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        # è§£æè¡Œä¸ºå­—å…¸\n",
    "        behavior_df = product_stats['behavior_type'].apply(pd.Series)\n",
    "        product_stats = pd.concat([product_stats.drop('behavior_type', axis=1), behavior_df], axis=1)\n",
    "        \n",
    "        # è®¡ç®—è½¬åŒ–ç‡\n",
    "        product_stats['view_to_cart'] = product_stats['carts'] / product_stats['views'].replace(0, 1) * 100\n",
    "        product_stats['view_to_purchase'] = product_stats['purchases'] / product_stats['views'].replace(0, 1) * 100\n",
    "        \n",
    "        # å•†å“åˆ†å±‚\n",
    "        product_stats['product_tier'] = pd.qcut(\n",
    "            product_stats['views'], \n",
    "            4, \n",
    "            labels=['Dç±»å•†å“', 'Cç±»å•†å“', 'Bç±»å•†å“', 'Aç±»å•†å“']\n",
    "        )\n",
    "        \n",
    "        result = {\n",
    "            'å•†å“ç»Ÿè®¡': {\n",
    "                'æ€»å•†å“æ•°': int(df['item_id'].nunique()),\n",
    "                'æ€»ç±»ç›®æ•°': int(df['category_id'].nunique())\n",
    "            },\n",
    "            'å•†å“åˆ†å±‚': product_stats['product_tier'].value_counts().to_dict(),\n",
    "            'çƒ­é—¨å•†å“': {\n",
    "                'æœ€å¤šæµè§ˆ': product_stats.nlargest(5, 'views')[['views']].to_dict(),\n",
    "                'æœ€å¤šè´­ä¹°': product_stats.nlargest(5, 'purchases')[['purchases']].to_dict(),\n",
    "                'æœ€é«˜è½¬åŒ–': product_stats[product_stats['views'] > 100].nlargest(5, 'view_to_purchase')[['view_to_purchase']].to_dict()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def analyze_conversion_funnel_enterprise(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"ä¼ä¸šçº§è½¬åŒ–æ¼æ–—åˆ†æ\"\"\"\n",
    "        user_journeys = df.groupby('user_id')['behavior_type'].apply(list)\n",
    "        \n",
    "        funnel = {\n",
    "            'æµè§ˆç”¨æˆ·': sum(1 for journey in user_journeys if 'pv' in journey),\n",
    "            'åŠ è´­ç”¨æˆ·': sum(1 for journey in user_journeys if 'cart' in journey),\n",
    "            'æ”¶è—ç”¨æˆ·': sum(1 for journey in user_journeys if 'fav' in journey),\n",
    "            'è´­ä¹°ç”¨æˆ·': sum(1 for journey in user_journeys if 'buy' in journey)\n",
    "        }\n",
    "        \n",
    "        # è®¡ç®—è½¬åŒ–ç‡\n",
    "        conversions = {}\n",
    "        if funnel['æµè§ˆç”¨æˆ·'] > 0:\n",
    "            conversions['æµè§ˆâ†’åŠ è´­'] = funnel['åŠ è´­ç”¨æˆ·'] / funnel['æµè§ˆç”¨æˆ·'] * 100\n",
    "            conversions['æµè§ˆâ†’æ”¶è—'] = funnel['æ”¶è—ç”¨æˆ·'] / funnel['æµè§ˆç”¨æˆ·'] * 100\n",
    "            conversions['æµè§ˆâ†’è´­ä¹°'] = funnel['è´­ä¹°ç”¨æˆ·'] / funnel['æµè§ˆç”¨æˆ·'] * 100\n",
    "        \n",
    "        if funnel['åŠ è´­ç”¨æˆ·'] > 0:\n",
    "            conversions['åŠ è´­â†’è´­ä¹°'] = funnel['è´­ä¹°ç”¨æˆ·'] / funnel['åŠ è´­ç”¨æˆ·'] * 100\n",
    "        \n",
    "        result = {\n",
    "            'è½¬åŒ–æ¼æ–—': funnel,\n",
    "            'è½¬åŒ–ç‡': conversions,\n",
    "            'å…³é”®æŒ‡æ ‡': {\n",
    "                'æ•´ä½“è½¬åŒ–ç‡': conversions.get('æµè§ˆâ†’è´­ä¹°', 0),\n",
    "                'åŠ è´­è½¬åŒ–ç‡': conversions.get('åŠ è´­â†’è´­ä¹°', 0)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def analyze_temporal_patterns_enterprise(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"ä¼ä¸šçº§æ—¶é—´æ¨¡å¼åˆ†æ\"\"\"\n",
    "        result = {\n",
    "            'å°æ—¶æ¨¡å¼': {\n",
    "                'æ´»è·ƒæ—¶æ®µåˆ†å¸ƒ': df.groupby('hour').size().to_dict(),\n",
    "                'è´­ä¹°æ—¶æ®µåˆ†å¸ƒ': df[df['behavior_type'] == 'buy'].groupby('hour').size().to_dict()\n",
    "            },\n",
    "            'æ˜ŸæœŸæ¨¡å¼': {\n",
    "                'æ´»è·ƒæ—¥åˆ†å¸ƒ': df.groupby('weekday').size().to_dict(),\n",
    "                'è´­ä¹°æ—¥åˆ†å¸ƒ': df[df['behavior_type'] == 'buy'].groupby('weekday').size().to_dict()\n",
    "            },\n",
    "            'æ—¶é—´å¯¹æ¯”': {\n",
    "                'å‘¨æœ«vså·¥ä½œæ—¥æ´»è·ƒæ¯”': float(df[df['is_weekend'] == 1].shape[0] / df[df['is_weekend'] == 0].shape[0]),\n",
    "                'é«˜å³°vséé«˜å³°æ´»è·ƒæ¯”': float(df[df['is_peak_hour'] == 1].shape[0] / df[df['is_peak_hour'] == 0].shape[0])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def analyze_user_value_rfm(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"RFMç”¨æˆ·ä»·å€¼åˆ†æ\"\"\"\n",
    "        purchases = df[df['behavior_type'] == 'buy']\n",
    "        \n",
    "        if len(purchases) == 0:\n",
    "            return {'error': 'æ— è´­ä¹°æ•°æ®ï¼Œæ— æ³•è¿›è¡ŒRFMåˆ†æ'}\n",
    "        \n",
    "        # è®¾ç½®åˆ†ææ—¥æœŸ\n",
    "        analysis_date = df['date'].max()\n",
    "        \n",
    "        # è®¡ç®—RFM\n",
    "        rfm = purchases.groupby('user_id').agg({\n",
    "            'date': lambda x: (analysis_date - x.max()).days,  # Recency\n",
    "            'user_id': 'count',  # Frequency\n",
    "        }).rename(columns={'date': 'recency', 'user_id': 'frequency'})\n",
    "        \n",
    "        # Monetaryç”¨frequencyæ›¿ä»£ï¼ˆç¼ºä¹é‡‘é¢æ•°æ®ï¼‰\n",
    "        rfm['monetary'] = rfm['frequency']\n",
    "        \n",
    "        # RFMåˆ†ç®±\n",
    "        rfm['R_Score'] = pd.qcut(rfm['recency'], 4, labels=[4, 3, 2, 1])\n",
    "        rfm['F_Score'] = pd.qcut(rfm['frequency'].rank(method='first'), 4, labels=[1, 2, 3, 4])\n",
    "        rfm['M_Score'] = pd.qcut(rfm['monetary'].rank(method='first'), 4, labels=[1, 2, 3, 4])\n",
    "        \n",
    "        # RFMæ€»åˆ†\n",
    "        rfm['RFM_Score'] = rfm['R_Score'].astype(str) + rfm['F_Score'].astype(str) + rfm['M_Score'].astype(str)\n",
    "        rfm['RFM_Sum'] = rfm[['R_Score', 'F_Score', 'M_Score']].sum(axis=1)\n",
    "        \n",
    "        # ç”¨æˆ·åˆ†å±‚\n",
    "        def rfm_segment(row):\n",
    "            if row['RFM_Sum'] >= 11:\n",
    "                return 'é«˜ä»·å€¼ç”¨æˆ·'\n",
    "            elif row['RFM_Sum'] >= 8:\n",
    "                return 'æ½œåŠ›ç”¨æˆ·'\n",
    "            elif row['RFM_Sum'] >= 5:\n",
    "                return 'ä¸€èˆ¬ç”¨æˆ·'\n",
    "            else:\n",
    "                return 'ä½ä»·å€¼ç”¨æˆ·'\n",
    "        \n",
    "        rfm['segment'] = rfm.apply(rfm_segment, axis=1)\n",
    "        \n",
    "        result = {\n",
    "            'RFMåˆ†æ': {\n",
    "                'æ€»è´­ä¹°ç”¨æˆ·': int(len(rfm)),\n",
    "                'ç”¨æˆ·åˆ†å±‚åˆ†å¸ƒ': rfm['segment'].value_counts().to_dict(),\n",
    "                'å„å±‚ç‰¹å¾': {\n",
    "                    segment: {\n",
    "                        'å¹³å‡æœ€è¿‘è´­ä¹°': float(rfm[rfm['segment'] == segment]['recency'].mean()),\n",
    "                        'å¹³å‡è´­ä¹°æ¬¡æ•°': float(rfm[rfm['segment'] == segment]['frequency'].mean())\n",
    "                    }\n",
    "                    for segment in rfm['segment'].unique()\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def analyze_inventory_optimization(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"åº“å­˜ä¼˜åŒ–åˆ†æ\"\"\"\n",
    "        product_sales = df[df['behavior_type'] == 'buy'].groupby('item_id').size()\n",
    "        \n",
    "        if len(product_sales) == 0:\n",
    "            return {'error': 'æ— é”€å”®æ•°æ®ï¼Œæ— æ³•è¿›è¡Œåº“å­˜åˆ†æ'}\n",
    "        \n",
    "        # ABCåˆ†æï¼ˆå¸•ç´¯æ‰˜åˆ†æï¼‰\n",
    "        product_sales_sorted = product_sales.sort_values(ascending=False)\n",
    "        cumulative_pct = product_sales_sorted.cumsum() / product_sales_sorted.sum() * 100\n",
    "        \n",
    "        a_count = (cumulative_pct <= 80).sum()\n",
    "        b_count = ((cumulative_pct > 80) & (cumulative_pct <= 95)).sum()\n",
    "        c_count = (cumulative_pct > 95).sum()\n",
    "        \n",
    "        # å•†å“åˆ†ç±»\n",
    "        product_stats = df.groupby('item_id').agg({\n",
    "            'behavior_type': lambda x: {\n",
    "                'views': (x == 'pv').sum(),\n",
    "                'purchases': (x == 'buy').sum()\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        behavior_df = product_stats['behavior_type'].apply(pd.Series)\n",
    "        product_stats = pd.concat([product_stats.drop('behavior_type', axis=1), behavior_df], axis=1)\n",
    "        \n",
    "        product_stats['conversion_rate'] = product_stats['purchases'] / product_stats['views'].replace(0, 1) * 100\n",
    "        \n",
    "        # å•†å“è¡¨ç°åˆ†ç±»\n",
    "        def classify_product(row):\n",
    "            if row['purchases'] >= 100 and row['conversion_rate'] >= 5:\n",
    "                return 'çƒ­é”€å“'\n",
    "            elif row['purchases'] >= 50:\n",
    "                return 'å¸¸é”€å“'\n",
    "            elif row['purchases'] >= 10:\n",
    "                return 'ä¸€èˆ¬å“'\n",
    "            else:\n",
    "                return 'æ»é”€å“'\n",
    "        \n",
    "        product_stats['product_type'] = product_stats.apply(classify_product, axis=1)\n",
    "        \n",
    "        result = {\n",
    "            'ABCåˆ†æ': {\n",
    "                'Aç±»å•†å“(é‡ç‚¹ç®¡ç†)': int(a_count),\n",
    "                'Bç±»å•†å“(å¸¸è§„ç®¡ç†)': int(b_count),\n",
    "                'Cç±»å•†å“(ç®€åŒ–ç®¡ç†)': int(c_count),\n",
    "                'Aç±»å•†å“é”€å”®å æ¯”': 80.0,\n",
    "                'Bç±»å•†å“é”€å”®å æ¯”': 15.0,\n",
    "                'Cç±»å•†å“é”€å”®å æ¯”': 5.0\n",
    "            },\n",
    "            'å•†å“è¡¨ç°åˆ†ç±»': product_stats['product_type'].value_counts().to_dict(),\n",
    "            'åº“å­˜å»ºè®®': {\n",
    "                'çƒ­é”€å“': 'é«˜åº“å­˜ï¼Œé¢‘ç¹è¡¥è´§',\n",
    "                'å¸¸é”€å“': 'ä¸­ç­‰åº“å­˜ï¼Œå®šæœŸè¡¥è´§',\n",
    "                'ä¸€èˆ¬å“': 'ä½åº“å­˜ï¼ŒæŒ‰éœ€è¡¥è´§',\n",
    "                'æ»é”€å“': 'æ¸…ä»“å¤„ç†æˆ–ä¸‹æ¶'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def generate_enterprise_report(self, data_path: str, results: Dict) -> str:\n",
    "        \"\"\"ç”Ÿæˆä¼ä¸šçº§æŠ¥å‘Š\"\"\"\n",
    "        report_path = os.path.join(self.reports_dir, \n",
    "                                 f\"enterprise_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\")\n",
    "        \n",
    "        df_sample = pd.read_csv(data_path, nrows=10000)\n",
    "        \n",
    "        with open(report_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"# ç”µå•†æ•°æ®åˆ†æä¼ä¸šçº§æŠ¥å‘Š\\n\\n\")\n",
    "            f.write(f\"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"æ•°æ®æ ·æœ¬: {len(df_sample):,} è¡Œ\\n\\n\")\n",
    "            \n",
    "            # æ‰§è¡Œæ‘˜è¦\n",
    "            f.write(\"## ğŸ“‹ æ‰§è¡Œæ‘˜è¦\\n\\n\")\n",
    "            f.write(\"æœ¬æŠ¥å‘ŠåŸºäºç”µå•†å¹³å°ç”¨æˆ·è¡Œä¸ºæ•°æ®ï¼Œé‡‡ç”¨ä¼ä¸šçº§åˆ†ææ–¹æ³•ï¼Œæä¾›å…¨é¢çš„ä¸šåŠ¡æ´å¯Ÿå’Œæˆ˜ç•¥å»ºè®®ã€‚\\n\\n\")\n",
    "            \n",
    "            # å…³é”®å‘ç°\n",
    "            f.write(\"## ğŸ¯ å…³é”®å‘ç°\\n\\n\")\n",
    "            \n",
    "            key_findings = []\n",
    "            \n",
    "            if 'ç”¨æˆ·è¡Œä¸ºåˆ†æ' in results:\n",
    "                ub = results['ç”¨æˆ·è¡Œä¸ºåˆ†æ']\n",
    "                if 'åŸºç¡€ç»Ÿè®¡' in ub:\n",
    "                    f.write(f\"- **æ€»ç”¨æˆ·æ•°**: {ub['åŸºç¡€ç»Ÿè®¡']['æ€»ç”¨æˆ·æ•°']:,}\\n\")\n",
    "                    f.write(f\"- **æ—¥å‡æ´»è·ƒç”¨æˆ·**: {ub['åŸºç¡€ç»Ÿè®¡']['æ—¥å‡æ´»è·ƒç”¨æˆ·']:.0f}\\n\")\n",
    "            \n",
    "            if 'é”€å”®è¡¨ç°åˆ†æ' in results:\n",
    "                sa = results['é”€å”®è¡¨ç°åˆ†æ']\n",
    "                if 'é”€å”®æ¦‚è§ˆ' in sa:\n",
    "                    f.write(f\"- **æ€»è´­ä¹°æ¬¡æ•°**: {sa['é”€å”®æ¦‚è§ˆ']['æ€»è´­ä¹°æ¬¡æ•°']:,}\\n\")\n",
    "                    f.write(f\"- **è´­ä¹°ç”¨æˆ·æ•°**: {sa['é”€å”®æ¦‚è§ˆ']['è´­ä¹°ç”¨æˆ·æ•°']:,}\\n\")\n",
    "            \n",
    "            if 'è½¬åŒ–æ¼æ–—åˆ†æ' in results:\n",
    "                cf = results['è½¬åŒ–æ¼æ–—åˆ†æ']\n",
    "                if 'å…³é”®æŒ‡æ ‡' in cf:\n",
    "                    f.write(f\"- **æ•´ä½“è½¬åŒ–ç‡**: {cf['å…³é”®æŒ‡æ ‡']['æ•´ä½“è½¬åŒ–ç‡']:.2f}%\\n\")\n",
    "                    f.write(f\"- **åŠ è´­è½¬åŒ–ç‡**: {cf['å…³é”®æŒ‡æ ‡']['åŠ è´­è½¬åŒ–ç‡']:.2f}%\\n\")\n",
    "            \n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # è¯¦ç»†åˆ†æ\n",
    "            f.write(\"## ğŸ” è¯¦ç»†åˆ†æ\\n\\n\")\n",
    "            \n",
    "            for analysis_name, result in results.items():\n",
    "                f.write(f\"### {analysis_name}\\n\\n\")\n",
    "                \n",
    "                if 'error' in result:\n",
    "                    f.write(f\"**åˆ†æå¼‚å¸¸**: {result['error']}\\n\\n\")\n",
    "                    continue\n",
    "                \n",
    "                # ç®€åŒ–çš„JSONè¾“å‡º\n",
    "                f.write(\"```json\\n\")\n",
    "                try:\n",
    "                    f.write(json.dumps(result, indent=2, ensure_ascii=False)[:1500])\n",
    "                except:\n",
    "                    f.write(\"æ•°æ®æ ¼å¼å¤æ‚ï¼Œè¯¦æƒ…è¯·çœ‹åŸå§‹æ•°æ®\\n\")\n",
    "                f.write(\"\\n```\\n\\n\")\n",
    "            \n",
    "            # æˆ˜ç•¥å»ºè®®\n",
    "            f.write(\"## ğŸ’¡ æˆ˜ç•¥å»ºè®®\\n\\n\")\n",
    "            \n",
    "            strategic_recommendations = [\n",
    "                \"### 1. ç”¨æˆ·å¢é•¿ä¸ç•™å­˜æˆ˜ç•¥\\n\",\n",
    "                \"- **ç”¨æˆ·è·å–**: åŸºäºRFMåˆ†æï¼Œé’ˆå¯¹é«˜ä»·å€¼ç”¨æˆ·ç‰¹å¾ä¼˜åŒ–è·å®¢æ¸ é“\\n\",\n",
    "                \"- **ç”¨æˆ·æ¿€æ´»**: ä¼˜åŒ–æ–°ç”¨æˆ·å¼•å¯¼æµç¨‹ï¼Œæé«˜7æ—¥ç•™å­˜ç‡\\n\",\n",
    "                \"- **ç”¨æˆ·ç•™å­˜**: å»ºç«‹ç”¨æˆ·ç”Ÿå‘½å‘¨æœŸç®¡ç†ä½“ç³»ï¼Œå‡å°‘ç”¨æˆ·æµå¤±\\n\\n\",\n",
    "                \n",
    "                \"### 2. è½¬åŒ–ä¼˜åŒ–æˆ˜ç•¥\\n\",\n",
    "                \"- **æ¼æ–—ä¼˜åŒ–**: é’ˆå¯¹è½¬åŒ–ç‡ä½çš„ç¯èŠ‚è¿›è¡Œé‡ç‚¹ä¼˜åŒ–\\n\",\n",
    "                \"- **ç”¨æˆ·ä½“éªŒ**: ä¼˜åŒ–å•†å“è¯¦æƒ…é¡µå’Œè´­ä¹°æµç¨‹\\n\",\n",
    "                \"- **ä¸ªæ€§åŒ–æ¨è**: åŸºäºç”¨æˆ·è¡Œä¸ºå®ç°ä¸ªæ€§åŒ–å•†å“æ¨è\\n\\n\",\n",
    "                \n",
    "                \"### 3. å•†å“ä¸åº“å­˜æˆ˜ç•¥\\n\",\n",
    "                \"- **å•†å“ç­–ç•¥**: é‡ç‚¹æ¨å¹¿Aç±»å•†å“ï¼Œä¼˜åŒ–æ»é”€å•†å“\\n\",\n",
    "                \"- **åº“å­˜ä¼˜åŒ–**: åŸºäºABCåˆ†æä¼˜åŒ–åº“å­˜ç»“æ„\\n\",\n",
    "                \"- **ä¾›åº”é“¾ä¼˜åŒ–**: åŸºäºé”€å”®é¢„æµ‹ä¼˜åŒ–è¡¥è´§ç­–ç•¥\\n\\n\",\n",
    "                \n",
    "                \"### 4. è¿è¥æ•ˆç‡æˆ˜ç•¥\\n\",\n",
    "                \"- **æ—¶é—´ä¼˜åŒ–**: åœ¨é”€å”®é«˜å³°æ—¶æ®µé›†ä¸­è¿è¥èµ„æº\\n\",\n",
    "                \"- **è‡ªåŠ¨åŒ–è¿è¥**: å»ºç«‹è‡ªåŠ¨åŒ–è¥é”€å’Œåº“å­˜ç®¡ç†ç³»ç»Ÿ\\n\",\n",
    "                \"- **æ•°æ®é©±åŠ¨**: å»ºç«‹æ•°æ®é©±åŠ¨çš„å†³ç­–ä½“ç³»\\n\\n\",\n",
    "                \n",
    "                \"### 5. æŠ€æœ¯åˆ›æ–°æˆ˜ç•¥\\n\",\n",
    "                \"- **æ•°æ®åˆ†æå¹³å°**: å»ºç«‹ä¼ä¸šçº§æ•°æ®åˆ†æå¹³å°\\n\",\n",
    "                \"- **AIåº”ç”¨**: å¼•å…¥æœºå™¨å­¦ä¹ å’Œé¢„æµ‹æ¨¡å‹\\n\",\n",
    "                \"- **å®æ—¶ç›‘æ§**: å»ºç«‹å®æ—¶æ•°æ®ç›‘æ§å’Œé¢„è­¦ç³»ç»Ÿ\\n\"\n",
    "            ]\n",
    "            \n",
    "            for line in strategic_recommendations:\n",
    "                f.write(line)\n",
    "            \n",
    "            # å®æ–½è·¯çº¿å›¾\n",
    "            f.write(\"\\n## ğŸ—ºï¸ å®æ–½è·¯çº¿å›¾\\n\\n\")\n",
    "            \n",
    "            roadmap = [\n",
    "                \"### ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€ä¼˜åŒ–ï¼ˆ1ä¸ªæœˆï¼‰\\n\",\n",
    "                \"- ä¼˜åŒ–è½¬åŒ–æ¼æ–—è–„å¼±ç¯èŠ‚\\n\",\n",
    "                \"- åŸºäºé«˜å³°æ—¶æ®µè°ƒæ•´è¿è¥å®‰æ’\\n\",\n",
    "                \"- æ¸…ç†æ»é”€åº“å­˜\\n\\n\",\n",
    "                \n",
    "                \"### ç¬¬äºŒé˜¶æ®µï¼šä½“ç³»å»ºè®¾ï¼ˆ3ä¸ªæœˆï¼‰\\n\",\n",
    "                \"- å»ºç«‹ç”¨æˆ·åˆ†å±‚è¿è¥ä½“ç³»\\n\",\n",
    "                \"- æ„å»ºæ•°æ®ç›‘æ§çœ‹æ¿\\n\",\n",
    "                \"- ä¼˜åŒ–å•†å“æ¨èç³»ç»Ÿ\\n\\n\",\n",
    "                \n",
    "                \"### ç¬¬ä¸‰é˜¶æ®µï¼šæ·±åŒ–åº”ç”¨ï¼ˆ6ä¸ªæœˆï¼‰\\n\",\n",
    "                \"- å»ºç«‹é¢„æµ‹åˆ†ææ¨¡å‹\\n\",\n",
    "                \"- å®ç°ä¸ªæ€§åŒ–è¥é”€è‡ªåŠ¨åŒ–\\n\",\n",
    "                \"- æ„å»ºå®Œæ•´çš„ç”µå•†æ•°æ®åˆ†æå¹³å°\\n\\n\",\n",
    "                \n",
    "                \"### ç¬¬å››é˜¶æ®µï¼šåˆ›æ–°å‘å±•ï¼ˆ12ä¸ªæœˆï¼‰\\n\",\n",
    "                \"- æ¢ç´¢AIå’Œå¤§æ•°æ®åº”ç”¨\\n\",\n",
    "                \"- æ„å»ºæ™ºèƒ½å†³ç­–ç³»ç»Ÿ\\n\",\n",
    "                \"- å»ºç«‹è¡Œä¸šé¢†å…ˆçš„æ•°æ®åˆ†æèƒ½åŠ›\\n\"\n",
    "            ]\n",
    "            \n",
    "            for line in roadmap:\n",
    "                f.write(line)\n",
    "            \n",
    "            # é™„å½•\n",
    "            f.write(\"\\n## ğŸ“Š é™„å½•\\n\\n\")\n",
    "            f.write(\"- **åˆ†ææ–¹æ³•**: æœ¬æŠ¥å‘Šé‡‡ç”¨RFMåˆ†æã€ABCåˆ†æã€è½¬åŒ–æ¼æ–—åˆ†æç­‰ä¼ä¸šçº§åˆ†ææ–¹æ³•\\n\")\n",
    "            f.write(\"- **æ•°æ®èŒƒå›´**: åˆ†æåŸºäºXXXä¸‡è¡Œç”¨æˆ·è¡Œä¸ºæ•°æ®\\n\")\n",
    "            f.write(\"- **åˆ†æå·¥å…·**: Python + Pandas + Matplotlib\\n\")\n",
    "            f.write(\"- **æŠ¥å‘Šå‘¨æœŸ**: æœ¬æŠ¥å‘Šä¸ºå®šæœŸåˆ†ææŠ¥å‘Šï¼Œå»ºè®®æ¯æœˆæ›´æ–°ä¸€æ¬¡\\n\")\n",
    "        \n",
    "        print(f\"âœ… ä¼ä¸šæŠ¥å‘Šç”Ÿæˆå®Œæˆ: {report_path}\")\n",
    "        return report_path\n",
    "    \n",
    "    def create_professional_dashboard(self, data_path: str, results: Dict) -> str:\n",
    "        \"\"\"åˆ›å»ºä¸“ä¸šä»ªè¡¨æ¿\"\"\"\n",
    "        print(\"  åˆ›å»ºä¸“ä¸šå¯è§†åŒ–å›¾è¡¨...\")\n",
    "        \n",
    "        dashboard_dir = os.path.join(self.results_dir, 'enterprise_dashboard')\n",
    "        os.makedirs(dashboard_dir, exist_ok=True)\n",
    "        \n",
    "        df = pd.read_csv(data_path, nrows=100000)\n",
    "        \n",
    "        try:\n",
    "            # 1. é”€å”®è¶‹åŠ¿å›¾\n",
    "            plt.figure(figsize=(14, 8))\n",
    "            \n",
    "            # é”€å”®æ•°æ®\n",
    "            purchases = df[df['behavior_type'] == 'buy']\n",
    "            if len(purchases) > 0:\n",
    "                daily_sales = purchases.groupby('date').size()\n",
    "                \n",
    "                plt.subplot(2, 3, 1)\n",
    "                daily_sales.plot(kind='line', marker='o', linewidth=2, color='#2ca02c')\n",
    "                plt.title('æ—¥é”€å”®è¶‹åŠ¿', fontsize=12, fontweight='bold')\n",
    "                plt.xlabel('æ—¥æœŸ')\n",
    "                plt.ylabel('é”€å”®æ•°é‡')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # 2. è½¬åŒ–æ¼æ–—å›¾\n",
    "            plt.subplot(2, 3, 2)\n",
    "            if 'è½¬åŒ–æ¼æ–—åˆ†æ' in results:\n",
    "                cf = results['è½¬åŒ–æ¼æ–—åˆ†æ']\n",
    "                if 'è½¬åŒ–æ¼æ–—' in cf:\n",
    "                    funnel = cf['è½¬åŒ–æ¼æ–—']\n",
    "                    stages = list(funnel.keys())\n",
    "                    values = list(funnel.values())\n",
    "                    \n",
    "                    plt.barh(stages, values, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
    "                    plt.title('ç”¨æˆ·è½¬åŒ–æ¼æ–—', fontsize=12, fontweight='bold')\n",
    "                    plt.xlabel('ç”¨æˆ·æ•°é‡')\n",
    "            \n",
    "            # 3. ç”¨æˆ·è¡Œä¸ºåˆ†å¸ƒ\n",
    "            plt.subplot(2, 3, 3)\n",
    "            behavior_counts = df['behavior_type'].value_counts()\n",
    "            behavior_labels = ['æµè§ˆ', 'åŠ è´­', 'æ”¶è—', 'è´­ä¹°']\n",
    "            behavior_values = [\n",
    "                behavior_counts.get('pv', 0),\n",
    "                behavior_counts.get('cart', 0),\n",
    "                behavior_counts.get('fav', 0),\n",
    "                behavior_counts.get('buy', 0)\n",
    "            ]\n",
    "            \n",
    "            plt.pie(behavior_values, labels=behavior_labels, autopct='%1.1f%%', \n",
    "                   colors=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
    "            plt.title('ç”¨æˆ·è¡Œä¸ºåˆ†å¸ƒ', fontsize=12, fontweight='bold')\n",
    "            \n",
    "            # 4. 24å°æ—¶æ´»è·ƒåº¦\n",
    "            plt.subplot(2, 3, 4)\n",
    "            hourly_activity = df.groupby('hour').size()\n",
    "            plt.bar(hourly_activity.index, hourly_activity.values, color='skyblue', edgecolor='black')\n",
    "            plt.title('24å°æ—¶ç”¨æˆ·æ´»è·ƒåº¦', fontsize=12, fontweight='bold')\n",
    "            plt.xlabel('å°æ—¶')\n",
    "            plt.ylabel('æ´»è·ƒåº¦')\n",
    "            plt.xticks(range(0, 24, 2))\n",
    "            \n",
    "            # 5. ç”¨æˆ·åˆ†å±‚\n",
    "            plt.subplot(2, 3, 5)\n",
    "            if 'ç”¨æˆ·ä»·å€¼åˆ†æ' in results:\n",
    "                rfm = results['ç”¨æˆ·ä»·å€¼åˆ†æ']\n",
    "                if 'RFMåˆ†æ' in rfm and 'ç”¨æˆ·åˆ†å±‚åˆ†å¸ƒ' in rfm['RFMåˆ†æ']:\n",
    "                    segments = rfm['RFMåˆ†æ']['ç”¨æˆ·åˆ†å±‚åˆ†å¸ƒ']\n",
    "                    plt.bar(segments.keys(), segments.values(), \n",
    "                           color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
    "                    plt.title('ç”¨æˆ·ä»·å€¼åˆ†å±‚', fontsize=12, fontweight='bold')\n",
    "                    plt.xlabel('ç”¨æˆ·ç±»å‹')\n",
    "                    plt.ylabel('ç”¨æˆ·æ•°é‡')\n",
    "            \n",
    "            # 6. å•†å“ABCåˆ†æ\n",
    "            plt.subplot(2, 3, 6)\n",
    "            if 'åº“å­˜ä¼˜åŒ–åˆ†æ' in results:\n",
    "                inv = results['åº“å­˜ä¼˜åŒ–åˆ†æ']\n",
    "                if 'ABCåˆ†æ' in inv:\n",
    "                    abc = inv['ABCåˆ†æ']\n",
    "                    abc_data = {\n",
    "                        'Aç±»å•†å“': abc.get('Aç±»å•†å“(é‡ç‚¹ç®¡ç†)', 0),\n",
    "                        'Bç±»å•†å“': abc.get('Bç±»å•†å“(å¸¸è§„ç®¡ç†)', 0),\n",
    "                        'Cç±»å•†å“': abc.get('Cç±»å•†å“(ç®€åŒ–ç®¡ç†)', 0)\n",
    "                    }\n",
    "                    plt.pie(abc_data.values(), labels=abc_data.keys(), autopct='%1.1f%%',\n",
    "                           colors=['#ff7f0e', '#2ca02c', '#1f77b4'])\n",
    "                    plt.title('å•†å“ABCåˆ†æ', fontsize=12, fontweight='bold')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            dashboard_chart = os.path.join(dashboard_dir, 'enterprise_dashboard.png')\n",
    "            plt.savefig(dashboard_chart, dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"  âœ… ä¸“ä¸šä»ªè¡¨æ¿å›¾è¡¨: {dashboard_chart}\")\n",
    "            \n",
    "            # åˆ›å»ºHTMLä»ªè¡¨æ¿\n",
    "            html_path = self.create_enterprise_html_dashboard(dashboard_dir, dashboard_chart)\n",
    "            \n",
    "            return html_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸  ä»ªè¡¨æ¿åˆ›å»ºå¤±è´¥: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def create_enterprise_html_dashboard(self, dashboard_dir: str, chart_path: str) -> str:\n",
    "        \"\"\"åˆ›å»ºä¼ä¸šçº§HTMLä»ªè¡¨æ¿\"\"\"\n",
    "        html_path = os.path.join(dashboard_dir, 'enterprise_dashboard.html')\n",
    "        \n",
    "        html_content = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"zh-CN\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>ç”µå•†æ•°æ®åˆ†æä¼ä¸šä»ªè¡¨æ¿</title>\n",
    "    <style>\n",
    "        body {{\n",
    "            font-family: 'Microsoft YaHei', Arial, sans-serif;\n",
    "            margin: 0;\n",
    "            padding: 20px;\n",
    "            background-color: #f5f5f5;\n",
    "        }}\n",
    "        .header {{\n",
    "            text-align: center;\n",
    "            padding: 20px;\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            color: white;\n",
    "            border-radius: 10px;\n",
    "            margin-bottom: 30px;\n",
    "        }}\n",
    "        .dashboard-grid {{\n",
    "            display: grid;\n",
    "            grid-template-columns: repeat(auto-fit, minmax(600px, 1fr));\n",
    "            gap: 20px;\n",
    "            margin-bottom: 30px;\n",
    "        }}\n",
    "        .dashboard-card {{\n",
    "            background: white;\n",
    "            padding: 20px;\n",
    "            border-radius: 10px;\n",
    "            box-shadow: 0 2px 10px rgba(0,0,0,0.1);\n",
    "        }}\n",
    "        .dashboard-card h3 {{\n",
    "            color: #333;\n",
    "            border-bottom: 2px solid #667eea;\n",
    "            padding-bottom: 10px;\n",
    "        }}\n",
    "        .dashboard-card img {{\n",
    "            width: 100%;\n",
    "            height: auto;\n",
    "            border-radius: 5px;\n",
    "        }}\n",
    "        .kpi-container {{\n",
    "            display: grid;\n",
    "            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));\n",
    "            gap: 15px;\n",
    "            margin-bottom: 30px;\n",
    "        }}\n",
    "        .kpi-card {{\n",
    "            background: white;\n",
    "            padding: 15px;\n",
    "            border-radius: 10px;\n",
    "            text-align: center;\n",
    "            box-shadow: 0 2px 5px rgba(0,0,0,0.1);\n",
    "        }}\n",
    "        .kpi-value {{\n",
    "            font-size: 24px;\n",
    "            font-weight: bold;\n",
    "            color: #667eea;\n",
    "        }}\n",
    "        .kpi-label {{\n",
    "            font-size: 14px;\n",
    "            color: #666;\n",
    "            margin-top: 5px;\n",
    "        }}\n",
    "        .footer {{\n",
    "            text-align: center;\n",
    "            margin-top: 30px;\n",
    "            padding: 20px;\n",
    "            color: #666;\n",
    "            font-size: 14px;\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"header\">\n",
    "        <h1>ğŸ“Š ç”µå•†æ•°æ®åˆ†æä¼ä¸šä»ªè¡¨æ¿</h1>\n",
    "        <p>ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
    "        <p>ä¼ä¸šçº§æ•°æ®åˆ†æä¸æ´å¯Ÿ</p>\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"kpi-container\">\n",
    "        <div class=\"kpi-card\">\n",
    "            <div class=\"kpi-value\">æ•°æ®åˆ†æ</div>\n",
    "            <div class=\"kpi-label\">ä¼ä¸šçº§æ·±åº¦åˆ†æ</div>\n",
    "        </div>\n",
    "        <div class=\"kpi-card\">\n",
    "            <div class=\"kpi-value\">7å¤§æ¨¡å—</div>\n",
    "            <div class=\"kpi-label\">å®Œæ•´åˆ†æä½“ç³»</div>\n",
    "        </div>\n",
    "        <div class=\"kpi-card\">\n",
    "            <div class=\"kpi-value\">ä¸“ä¸šå¯è§†åŒ–</div>\n",
    "            <div class=\"kpi-label\">å†³ç­–æ”¯æŒå›¾è¡¨</div>\n",
    "        </div>\n",
    "        <div class=\"kpi-card\">\n",
    "            <div class=\"kpi-value\">æˆ˜ç•¥å»ºè®®</div>\n",
    "            <div class=\"kpi-label\">å¯æ‰§è¡Œæ–¹æ¡ˆ</div>\n",
    "        </div>\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"dashboard-grid\">\n",
    "        <div class=\"dashboard-card\">\n",
    "            <h3>ä¼ä¸šçº§ç»¼åˆåˆ†æä»ªè¡¨æ¿</h3>\n",
    "            <img src=\"{os.path.basename(chart_path)}\" alt=\"ä¼ä¸šä»ªè¡¨æ¿\">\n",
    "            <p>åŒ…å«é”€å”®è¶‹åŠ¿ã€è½¬åŒ–æ¼æ–—ã€ç”¨æˆ·è¡Œä¸ºã€æ—¶é—´æ¨¡å¼ã€ç”¨æˆ·åˆ†å±‚ã€å•†å“åˆ†æç­‰6ä¸ªå…³é”®è§†å›¾ã€‚</p>\n",
    "        </div>\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"dashboard-grid\">\n",
    "        <div class=\"dashboard-card\">\n",
    "            <h3>ğŸ“‹ åˆ†ææ¨¡å—</h3>\n",
    "            <ul>\n",
    "                <li>ç”¨æˆ·è¡Œä¸ºåˆ†æ</li>\n",
    "                <li>é”€å”®è¡¨ç°åˆ†æ</li>\n",
    "                <li>å•†å“ç»„åˆåˆ†æ</li>\n",
    "                <li>è½¬åŒ–æ¼æ–—åˆ†æ</li>\n",
    "                <li>æ—¶é—´æ¨¡å¼åˆ†æ</li>\n",
    "                <li>ç”¨æˆ·ä»·å€¼åˆ†æ(RFM)</li>\n",
    "                <li>åº“å­˜ä¼˜åŒ–åˆ†æ</li>\n",
    "            </ul>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"dashboard-card\">\n",
    "            <h3>ğŸ¯ æ ¸å¿ƒä»·å€¼</h3>\n",
    "            <ul>\n",
    "                <li>æ•°æ®é©±åŠ¨çš„å†³ç­–æ”¯æŒ</li>\n",
    "                <li>ç”¨æˆ·è¡Œä¸ºæ·±åº¦æ´å¯Ÿ</li>\n",
    "                <li>å•†å“ç­–ç•¥ä¼˜åŒ–å»ºè®®</li>\n",
    "                <li>åº“å­˜ç®¡ç†ä¼˜åŒ–æ–¹æ¡ˆ</li>\n",
    "                <li>è½¬åŒ–ç‡æå‡ç­–ç•¥</li>\n",
    "                <li>ç”¨æˆ·ä»·å€¼åˆ†å±‚ç®¡ç†</li>\n",
    "            </ul>\n",
    "        </div>\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"footer\">\n",
    "        <p>Â© 2024 ç”µå•†æ•°æ®åˆ†æä¼ä¸šç³»ç»Ÿ | åŸºäºå¤§æ•°æ®åˆ†æç”Ÿæˆ | ä»…ä¾›å†…éƒ¨å†³ç­–å‚è€ƒ</p>\n",
    "        <p>å»ºè®®ç»“åˆå…·ä½“ä¸šåŠ¡æƒ…å†µè°ƒæ•´å®æ–½ç­–ç•¥</p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "        \n",
    "        with open(html_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(html_content)\n",
    "        \n",
    "        print(f\"  âœ… ä¼ä¸šçº§HTMLä»ªè¡¨æ¿: {html_path}\")\n",
    "        return html_path\n",
    "\n",
    "def main():\n",
    "    \"\"\"ä¸»å‡½æ•°\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"ğŸ¢ ä¼ä¸šçº§ç”µå•†æ•°æ®åˆ†æç³»ç»Ÿå¯åŠ¨\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # åˆ›å»ºç³»ç»Ÿå®ä¾‹\n",
    "    system = EnterpriseEcommerceSystem()\n",
    "    \n",
    "    # è¿è¡Œä¼ä¸šçº§åˆ†æ\n",
    "    try:\n",
    "        report_path = system.run_enterprise_analysis()\n",
    "        \n",
    "        if report_path:\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"âœ… ä¼ä¸šçº§åˆ†ææˆåŠŸå®Œæˆï¼\")\n",
    "            print(\"=\"*70)\n",
    "            print(\"\\nğŸ¯ ä½ å¯ä»¥ï¼š\")\n",
    "            print(f\"1. æŸ¥çœ‹ä¼ä¸šæŠ¥å‘Š: {report_path}\")\n",
    "            print(\"2. æ‰“å¼€HTMLä»ªè¡¨æ¿æŸ¥çœ‹å¯è§†åŒ–\")\n",
    "            print(\"3. åŸºäºæˆ˜ç•¥å»ºè®®åˆ¶å®šä¸šåŠ¡è®¡åˆ’\")\n",
    "            print(\"4. ä¸å›¢é˜Ÿåˆ†äº«åˆ†ææˆæœ\")\n",
    "        else:\n",
    "            print(\"\\nâŒ åˆ†ææ‰§è¡Œå¤±è´¥\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ ç³»ç»Ÿæ‰§è¡Œå¼‚å¸¸: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ ç³»ç»Ÿæ‰§è¡Œå®Œæˆ\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13d292e2-8e08-4b43-9e3b-f5cad9ad9698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹æ•´ç†é¡¹ç›®ç»“æ„...\n",
      "âœ… é¡¹ç›®æ•´ç†å®Œæˆï¼\n",
      "\n",
      "ç§»åŠ¨çš„é¡¹ç›®:\n",
      "  - UserBehavior.csv â†’ data/raw/\n",
      "  - processed_data â†’ data/processed/\n",
      "  - enterprise_processed â†’ data/processed/\n",
      "  - enterprise_results â†’ data/processed/\n",
      "  - enterprise_reports â†’ reports/\n",
      "  - analysis_charts â†’ reports/\n",
      "  - Untitled.ipynb â†’ notebooks/\n",
      "  - åˆ é™¤: .ipynb_checkpoints\n",
      "\n",
      "ğŸ“ æ–°ç»“æ„:\n",
      "./\n",
      "  advanced_analysis_report.md\n",
      "  business_analysis_report.md\n",
      "  README.md\n",
      "  data/\n",
      "    processed/\n",
      "      enterprise_processed/\n",
      "        enterprise_cleaned.csv\n",
      "      enterprise_results/\n",
      "        enterprise_dashboard/\n",
      "          enterprise_dashboard.html\n",
      "          enterprise_dashboard.png\n",
      "      processed_data/\n",
      "        user_behavior_clean.csv\n",
      "    raw/\n",
      "      UserBehavior.csv\n",
      "  notebooks/\n",
      "    Untitled.ipynb\n",
      "  reports/\n",
      "    analysis_charts/\n",
      "      behavior_distribution.png\n",
      "      conversion_funnel.png\n",
      "      hourly_sales_trend.png\n",
      "      top_products.png\n",
      "      user_segmentation.png\n",
      "    enterprise_reports/\n",
      "      enterprise_report_20260131_224101.md\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1270f14-4427-4050-8ce4-f03dfdcbb2a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
